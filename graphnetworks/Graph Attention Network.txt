[Attention까지의 발달사]

1.	어텐션은 주목할 부분도 알아서 학습하자라는 철학을 가짐.
2.	GAN은 노드,엣지로 이루어진 Graph 데이터의 중요 노드에 가중치를 부여하는 Attention 메커니즘을 사용해 학습하는 Network 모델.
3.	어텐션이란?
	1) RNN은 시계열 데이터 학습에 용이함. 하나의 시퀀스에 대해 히든 스테이트를 생성하는 모델.
	2) seq2seq은 RNN 인코더와 디코더를 사용해, 인코더의 아웃풋으로 하나의 고정된 context vector로 표현하고, 
		디코더에서는 이 context vector를 받아 하나씩 순차적으로 토큰을 예측함.
	3) 인풋 시퀀스가 길어지면, 롱텀 디펜던시 문제와 vanishing/exploding gradient 문제(가중치를 중복해 곱하기 때문에 도중에 지나치게 크거나 작은 값이 나오면 기울기가 터지거나 소실)
	
	4) 모든 정보를 하나의 고정된 길이의 벡터로 인코딩 해야하는 부담으로부터 인코더를 구제해주자! 라는 아이디어로 등장한 것이 어텐션임. 
	
	5) 어텐션 개념 이해를 위해 dictionary 자료구조를 이용함. query, key, value가 등장함.
		query와 key의 유사도를 구하고(유사도 함수를 사용) 그에 해당하는 value를 반환하게 되는 구조를 연상. 
		기존의 자료구조에서는 query와 key가 같으면 유사도가 1, 다르면 0으로만 출력되지만, 
		attention에서는 주목해야할 정도에 따라 달라지며 이 정도를 주어진 태스크 해결을 위해 조정해 나가는 것을 생각할 수 있음.
		이렇게 구한 유사도 벡터를 value 벡터와 곱해주면 유사도가 높은 value의 영향을 많이 받은 벡터값이 출력됨. 
		이 결과값을 총합하여 최종 결과로 사용
	
	6) 요약하자면, 어텐션은
		1. key,value의 유사도 측정
		2. 유사도와 value를 곱한다.
		3. 2. 값의 총합(summation)을 리턴한다.
		이 과정을 따른다.
		-> sum(sim(query, key)*value)
		
		이를 어텐션스러운 수식으로 바꾸면,
		-> A(q,K,V)=sum(softmax(f(K,q))*V)
		즉, q와 K의 유사도를 구한 후 V와의 weighted sum을 구함.
		
	7) attention score: q,K와위 유사도를 구한 후 Value에 곱해지는 가중치. 
		이를 토대로 weighted sum이 이루어짐
	8) 여기서 q,K,V는 벡터/매트릭스/텐서가 될 수 있음. 또한 다양한 유사도 함수를 사용 가능.
	
4.	어텐션의 활용
	1) 기계 번역
		q: decoder의 t+1기 hidden state 
		K,V: encoder의 t기(까지 각 시점마다의) hidden state
		feature: sum(softmax(sim(q,K))*V)
		
		이처럼 각 시점마다의 context vector가 생성됨.
	
	2) RNN문서 분류에서의 어텐션 활용
		q: 학습가능한 파라미터 벡터(context vector) 
		K,V: RNN의 hidden state h_i
		feature: document vector
	
5.	유사도 함수
	1) addictive/concat (Graph Neural Net에서 사용하는 유사도 함수)
	2) dot product
	3) scaled-dot product (transformer)
	
6.	RNN & CNN
	1) RNN 네트워크의 종류
		- biRNN
		- hierarchical Attention Network
	
	2) CNN 구조의 적용
		- textCNN
		- character level CNN
	
	3) RNN/CNN의 한계
		- RNN은 병렬 컴퓨팅 불가, 연산 시간 및 복잡도가 높음, 기울기 소실, 롱텀디펜던시 문제(이 두 문제는 어텐션 적용해도 존재) 등을 겪음
	
		- CNN은 long path length between long-range dependency 문제. 
		윈도우 사이즈를 넘어서는 간격으로 중요한 토큰들이 배치되어 있다면, 이 두 토큰의 정보를 학습하기 위해서는 깊은 Conv layer를 쌓아야함
	
7.	Attention to self-attention (Attention is all you need, Transformer)
	: RNN/CNN을 사용하지 않고, Attention만 가지고 feature representation 해보자!
	
	1) K,Q,V 모두 hidden state of word embedding vector X와 각각(k,q,v)에 대한 weight matrix W의 곱으로 표현 됨
	2) scaled dot-product를 유사도 함수로 사용
	3) Multi-head attention

8.	Transformer
	1) dot product를 사용한 이유:
		계산효율적이고,행렬 연산에 최적화된 방식이기 때문. 텐서 연산으로 쉽게 계산 가능
	2) scale
		key의 차원에 루트를 취해준 값으로 scaling 해줌. k가 너무 커짐에 따라 softmax 값이 지나치게 작은 기울기를 갖게끔 되는 것을 막기 위함.
	3) multi-head attention
		k,q,v에 대한 weight matrix W를 head의 개수만큼 쪼개어 셀프 어텐션 계산을 해준 뒤 마지막에 concat한 뒤 Wo를 곱해 선형변환을 취해 줌.
	4) 성과
		계산 효율성 증대, 병렬화 가능, long range dependency 학습 가능, 해석 가능성 증가


[Graph Neural Network]

1. Graph Neural Network
	1) Node(vertex)와 edge로 구성된 자료구조. 
		node는 시스템 내의 원소를, edge는 node간의 상호작용 혹은 상관관계를 나타냄
	2) 유클리드 공간에 표현할 수 없음. 
		(e.g. 3D mesh, social network, molecular structure)
		
2.	Node의 matrix representation
	1) node feature matrix
		각 행은 그래프의 노드를 의미하고, 각 열은 노드의 특징(feature)를 의미함.
	2) adjacency matrix
		각 행과 열은 노드를 의미하고, 그 안의 값은 연결 상태를 나타냄. 
		노드의 수가 n개 일때, 전체 매트릭스의 크기는 n*n이 됨.
	
3.	Edge의 matrix representation
	1) undirected matrix
		방향이 없고, 연결 상태만을 함축함. 
		대칭적인 형태를 가짐.
	2) directed matrix
		비대칭적인 형태를 가짐. 상행선은 있고 하행선은 없는 경우 등을 생각할 수 있음.
	3) identity matrix를 더하면 자기자신과의 연결성을 표현할 수 있음.
	4) weighted matrix
		관계의 무게를 표현할 수 있음. 고속도로의 톨비를 생각하면 됨.
	
4.	GNN tasks
	1) node level: 노드 분류, 노드 회귀
	2) edge level: 엣지 분류
	3) graph level: 그래프 분류(텍스트 분류 등)
	4) 각 노드가 이미지, 텍스트인 경우도 있음. 이미지 안의 개체간의 관계의 그래프, 텍스트 안의 토큰들의 관계에 대한 그래프를 생각할 수 있음.
	
5.	GNN learning process
	1) 그래프는 sequential data와 다름
		- 정해진 순서가 없음
		- 여러개의 노드가 연결될 수 있음
		- 다양한 그래프 구조를 가질 수 있음
		
	2) 그래프 인코딩시 고려할 점\n- 시간 순서가 없으므로 동시에 넣어야함
		- 엣지를 따라서 정보가 이동함
		- 타겟노드는 여러 노드에 영향을 받음
		
6.	GNN 학습 절차(그래프 구조를 반영한 노드 피처 업데이트)
		- 그래프 정보가 입력으로 들어가서 출력으로 나옴
	1) Aggregate/message passing
		- message passing은 때때로 aggregate와 combine을 합친 절차로 표현되기도 함
	2) Combine/update
	3) Readout

8.	GNN notation
	1) graph=G(A,X)
	2) h^(k)_v : k번째 그래프 레이어의 히든 임베딩 노드 v
	3) v = target node
	4) N(v) = neighbor node of v
	5) u = N(v)의 원소 중 하나
	
9.	k-1번째 그래프 레이어와 k번째 그래프 레이어를 예시로 들어 학습 과정 설명
	1) aggregate
		이웃 노드들의 히든 임베딩을 aggregate해 요약된 형태로 만든다.
	2) combine
		k-1기의 target node의 hidden state(임베딩 값)와 앞서 요약한 aggregated 정보를 결합한다. 이로써 k기의 target node hidden state로 업데이트함
	3) readout
		k 시점의 모든 node들의 hidden state들을 결합하여 하나의 요약된 정보로 만드는 과정
		-> graph level의 task를 수행할때만 필요, 그 외에는 할 필요 없는 절차임
		
10.	Stacking CNN layer
	1) 일반적으로 딥러닝 모델은 여러 layer를 쌓아 심층적인 구조를 가짐. 이를 통해 여러 이점을 취할 수 있음
	2) CNN: 인지영역(receptive field)을 넓힐 수 있음. 더 넓은 영역의 정보를 얻을 수 있음
	3) GNN: 그래프의 hop(노드간의 정보 이동)을 늘릴 수 있음. 여러 다리 건너서의 노드간의 정보 교환을 표현할 수 있게 됨
	
11.	GNN variants
	1) aggregate, combine을 어떻게 정의하냐에 따라 다양한 방식의 모델을 사용 가능
	2) aggregate, combine 함수는 모두 미분 가능해야함.
	
12.	GNN
	1) Aggregation: 단순한 summation으로만 정보를 요약함
	2) Combine단계에서는, aggregate한 정보를 W_neigh로 선형변환하고, 자기 자신의 hidden state 정보를 W_self로 선형변환한 뒤 더해주는 연산으로 처리, 그 뒤 Relu activation
	
13.	GNN(self-loop)
	1) aggregation 과정에서, 자기 자신의 정보를 요약하는 것이 필요함. 
	2) W_neigh=W_self=W
	3) agg와 comb과정의 경계가 모호해짐. agg할 때 자기 자신의 정보도 함께 결합하도록 처리해줌. 
	
14.	GCN(Graph Convolution Network)
	1) 그래프가 너무 커지면, 노드의 차수가 커짐에 따라 학습이 불안정해지고 민감해짐.
		-> 차수(degree): 노드에 연결된 노드의 개수.
	2) normalized aggregation
		노드의 차수를 반영해서 normalize해줘야 한다! 
15.	GGN(Gated Graph Neural Network)
	: 너무 깊은 레이어를 쌓으면 오버피팅과 기울기 소실 문제를 야기할 수 있음-> gated cell을 사용해서 해결
	1) agg: 단순 합으로 처리
	2) combine: GRU 셀을 사용, activation 대신 GRU가 추가되었다고 생각할 수 있음.


16.	GraphSAGE
	1) 노드마다 중요도와 순서가 있으므로 이를 고려해야한다. 노드 샘플링과 inductive/transductive에 대해 구분도 필요함
	2) agg
		크게 3가지, 
		- mean agg(이웃 정보 결합 과정에서, 차수로 나누어 주어 normalize), 
		-LSTM agg(순서가 없으므로 random permutation 적용) 
		- pooling agg(pooling weight를 이웃 노드의 hidden과 곱해줌. 여기에는 mean pooling, max pooling(이게 더 나음) 두가지가 있음)
	3) combine
		타겟 노드의 히든값과 agg 정보를 concat한 뒤에 Weight matrix를 곱해 준 뒤 target node를 업데이트함.
	4) 층을 깊이 쌓으면 학습이 잘 되지 않는 현상을 보완하기 위해,  residual connection과 skip connection을 사용
	
17.	GAN(Graph Attention Network)
	1) agg 함수
		attention 함수로, t-1기의 타겟 노드를 포함한 이웃 노드들의 히든 임베딩을 대상으로 attention 수행
		이 때, K,Q,V는 모두 같고, t-1기의 이웃 노드들의 히든 임베딩과 같음
		
		- similarity function
		트랜스포머의 scaled dot-product를 사용하지는 않았고, 바흐다나호가 Machine translation에서 사용한 1 layer feed forward NN을 사용
		
		- t-1기의 target node의 hidden embedding을 KQV로 공유하기 때문에, 같은 웨이트를 공유?
		target node가 1번, neighbor node 2,4번이 있을 때 
		1번(여기서는 query 역할) hidden embedding과 2번(여기서는 key 역할)의 hidden embedding을 concat하여 
		1 layer FFN에 넣고 leakyRelu 통과시킨 값을 유사도로 사용
		
		4번 노드에 대해서도 유사도 계산 똑같은 방식으로 적용
		이렇게 구한 유사도 값을 node size**2의 matrix로 표현함. 직접적인 연결이 없는 노드들은 빈 칸으로 남음. 
		이는 정보를 얼마나 전달할 것인가에 대한 attention score를 구했다고 할 수 있음. 
		즉 e_12를 1번 노드를 업데이트 할 때 2번에서 얼마나 정보를 가져올지를 나타냄.
		그러나 빈 칸을 반영해서 attention을 구하는 코드 구현이 어려우므로, 일단 전체 matrix를 구한 뒤 node 연결이 없는 경우를 아주 작은 숫자로 replace 해줌. 
		이 값이 softmax를 통과하면 0에 근사한 값이 나올 것. 
		이 방식은 transformer decoder의 masked self-attention을 계산하는 방식과 매우 유사함
	
	2) combine
		이제 구해낸 attention score와 각 이웃 노드의 hidden embedding 값(즉, V) weight sum 해주어서, t기의 target node embedding을 업데이트 함.
		
	3) multi-head attention
		concat 혹은 average로 통합해줌.
