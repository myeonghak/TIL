Image GPT

	1.	gpt와 같은 방식으로 트랜스포머 블록을 쌓아서 모델을 구성함. 
		Autoregressive한 방식으로, 지금까지의 픽셀을 모두 받았을 때 다음 픽셀을 예측하는 문제를 해결함
		(BERT의 경우 AutoEncoder 방식으로 텍스트의 중간에 구멍을 뚫어 예측하는 방식을 사용. bidirectional하게 주어진 인풋 시퀀스를 모두 확인하며 중간을 예측한다는 차이가 있음)
	
	2.	기존의 224^2*3픽셀은 너무 크므로 32^2처럼 줄여서 학습시킴.
	
	3.	linear probe:
		pre-train된 representation은 그대로 두고, 그 임베딩으로 다운스트림 태스크를 수행하는 것. 
		이는 얼마나 pre-trained representation이 잘 학습되었는지를 파악할 수 있음.
		
	4.	실험 결과, last layer가 아닌 intermediate layer가 linear probe에 최적의 결과를 낳음! 
		- 이는 시작단에서는 CNN같은 transformation이 일어나고, 마지막 레이어에서는 “다음 픽셀을 맞춘다”는 특정 태스크에 집중하기 때문으로 생각됨
		
	5.	모델이 클수록 더욱 좋은 일반화된 표현 능력을 가짐.