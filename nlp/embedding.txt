[Embeddings: A matrix of meaning]

https://medium.com/@Petuum/embeddings-a-matrix-of-meaning-4de877c9aa27

Training an Embedding Matrix
1. 다른 학습과는 달리, target으로 삼을 ground-truth가 없음. 따라서 다른 목표를 가지고 학습을 시키고 그로부터 부산물로써 임베딩을 만들어 내기를 바랄수밖에 없음.

한가지 방법은 단어 그 자체에서 단어의 맥락을 추측하도록 모델을 디자인하는 것임. 이 방법은 모델이 제대로 추측했는지를 항상 검증하기 때문에 유용한 반면, 임베딩이 적절한지를 검증할 수 없다는 점이 한계임.

임베딩 매트릭스는 맥락 추측 모델에서 랜덤하게 초기화되어 파라미터로 설정됨. 비용은 맥락 임베딩을 얼마나 모델이 가깝게 추측했는지를 봄으로써 계산할 수 있음. 그 뒤 전체 모델은 경사 하강을 통해 학습 가능.
이는 CBOW 방법임.


임베딩 매트릭스 배포하기

임베딩 매트릭스를 올바르게 학습하는 것은 대부분의 NLP 머신러닝 모델의 필수 조건임.
그러나 임베딩 매트릭스를 다루는 일은 엔지니어링적인 어려움을 야기함.
매트릭스는 상당히 커서 텐서 기반 프레임워크가 설계된 형식의 가정을 따르지 않음.
이미지 처리 모델에서는, 파라미터들이 보통 dense함. (전체 매트릭스가 매 스텝마다 학습되고 업데이트 됨)
그러나 임베딩 매트릭스를 학습할때는, 트레이닝 데이터셋의 현재 context window에 특정된 행들(단어들)만을 학습하게 됨.
거의 대부분의 매트릭스는 주어진 학습 스텝동안 업데이트 되지 않음.

한 노드의 경우에서, 전체 매트릭스를 저장할 필요가 있기 때문에, 작은 결과이지만, 학습 모델을 배포할 때는, 중요한 암시를 가짐.

일반적으로, 딥러닝에서, 데이터 병렬처리를 사용한 클러스터를 통해 모델을 배포하는데, 
병렬처리란 클러스터 내의 각각의 작업 노드들이 같은 모델을 통해 작동하지만, 다른 데이터로 작동하는 것을 의미함.

그렇다면, 매트릭스의 특정 행들을 어떻게 네트워크 전반에 효율적으로 전송할 수 있을까?

우리의 연구 경험에 따라 다음의 경험법칙이 나왔음.
- 데이터를 복사할때, 하나씩 하는것보다 벌크로 하는게 유용함. 전송할 100개의 행이 있다고 하면, 모두를 통합한 채로 1번 전송하는 것이 100번 쪼개어 보내는 것보다 나음. 이는 메모리와 GPU 전송도 마찬가지.

- 통합할때 메모리 오버헤드를 아끼기 위해, 새로운 버퍼 없이, 제 자리에서 하는 것이 좋음.

- 행을 통합한 이후, 원래 행이 나온 부분과 커뮤니케이션 해야함. 이는 두 방법에 의해 이루어짐. 인덱스의 배열 혹은 비트맵.
	최적의 방식은 복사될 필요가 있는 행의 퍼센티지에 달려있음. 또한 두 방법 모두(인덱스 배열 / 비트맵) 압축 가능함.

- 실제로는, 몇몇 행은 다른 행보다 더 빈번하게 등장함: 언어에 무관하게, 단어는 지프 분포를 따름. 균일 분포가 아님.
	이는 당신의 의사소통을 쪼개려고 의도할 경우 어떤 함의를 가짐.
	예를 들어, 임베딩 행렬이 가장 빈번한 단어의 순서로부터 역순으로 정렬되고 이 행렬을 4개의 섹션으로 쪼개려고 할 때,첫번째 부분은 나머지 부분보다 훨씬 빈번하게 업데이트 된다는 것이다.

이상적으로는, 행렬의 행들을 무작위화 하여 행렬을 섹션으로 쪼개는 것이 특정 섹션을 다른 섹션보다 더 빈번하게 업데이트 하는 일이 일어나지 않도록 할수도 있음.
머신러닝 시스템을 설계하는 것은 항상 trade off를 포함하기 마련임. 프레임워크가 더 강력해질수록, 사용은 더욱 어려워짐.
임베딩 행렬은 여느 다른 딥러닝 파라미터처럼 보편적인 텐서 틀에 적합할수 있으나, 이를 다른 파라미터처럼 다루게 되면 이 행렬의 sparsity에서부터 이점을 취할 능력을 잃게 됨.
(이 행렬이 전부 한번에 업데이트 될 필요는 없다는 사실을)

만일 최소한의 수정된 정보만을 전송한다면
- 임베딩 행렬의 행들은 주어진 스텝 하에 업데이트 될 것
- 더 적은 정보를 소통하면서도 전체 작업을 분배할 수 있음. 이는 효과적으로 전체 태스크를 병렬화하여 효율성을 늘릴 수 있음. 임베딩 행렬은 머신러닝에서 유일한 특별 파라미터(special-case parameter)는 아님.
- sparsity는 다양한 형태로 나타날 수 있음. 일반적으로, 파라미터를 3개의 카테고리로 나눌 수 있음.:
	sparse하게 저장되고, 소통됨 (비트맵과 유사)
	dense하게 저장되고, sparse하게 소통됨 (임베딩 행렬)
	혹은 dense하게 저장되고 소통됨 (전통적 텐서들)
머신러닝 알고리즘의 큰 범주의 문제를 풀 시스템을 설계할 때, 이 모든 세가지 family를 지원하는 것이 필요함.

