Graph representation 1
metapath2vec 논문

1. 추천시스템의 큰 세 개의 구성
	1) sequential recommendation
	: 순서 정보, 이 물건을 산 뒤 다음에 어떤 것을 산다.
	
	2) embedding
	: 상품 혹은 고객의 feature(부가 정보)를 embedding한다
	
	3) learning method
	: adverserial training, 구별하기 어려운 negative sample을 사용해 학습을 어렵게 만듦으로써 일반화 성능을 올림

2. Geometry Learning
	- 데이터의 정보가 보존되는 공간에서 데이터의 특성을 학습하는 것
	: 비유클리드 데이터를 활용하기 위해, 데이터간의 관계(relation)를 학습함. 이 과정에서 데이터가 가지고 있는 구조적 정보의 변화를 만들어냄.
	예를 들어 네트워크, 트리, 분자구조, 매니폴드 등이 있음. 
	
	1) 트리
	부모 노드가 자식 노드와 대부분 관련이 있고, 자식 노드는 개별적인 특성을 가짐
	(하이퍼볼릭 공간에서의 임베딩)

	2) 네트워크 
	그래프는 노드간의 관계 및 구조, 연결성에서 오는 정보를 보존하고자 함.
	
	3) 분자구조
	분자식을 문자열로 표현하면 구조적 정보의 손실이 발생
	유클리드 기반 데이터는 d차원의 공간에서 표현되어, 유사한 것들은 거리가 가깝게 존재하는 식의 데이터를 의미
	n차원의 데이터를 n차원 그대로 두지 않는 이유는, 차원의 저주와 오버피팅 문제.
	특정 차원의 공간에 맵핑함으로써 더 유용한 정보를 활용할 수 있음

3. Heterogeneous Information Network
	같은 노드가 한가지 이상의 다양한 의미를 내포하는 경우
	
	1) 같은 노드지만 다양한 관계를 가짐
	(페이스북 친구, 트위터 친구..)

	2) 노드 간의 관계를 통해 다양한 관계 추출 가능
	(같은 배우가 출연한 작품, 같은 감독의 작품)
	
	3) 다양한 type의 노드를 통해 이웃을 다양하게 정의
	(배우기준 영화 1의 이웃: 영화2, 감독 기준 영화1의 이웃:영화3..)


4. HIN의 방법론
	:path기반, 임베딩 기반

	1) path 기반
	path상에 자주 등장하는 애들은 가까운 애들이다! 딥러닝 발전 이후 많은 연구가 진행됨. 


	2) 임베딩 기반 방식 -> 다음 장에서



5. 임베딩 방식
	1) transE
	: loss-> head(이병헌)+relation(관계)-tail(광해)=0
	같이 자주 등장하는 node를 유사한 것으로 간주하고, 가까운 공간에 임베딩되도록 학습. pos 샘플이 등장하면 로스가 적도록, neg 샘플이 등장하면 로스가 크도록 학습.
	그러나 학습되는 relation의 정보가 반영되지 않음.
	
	2) transR
	:‘다른 관계를 갖는 엔티티들은 다른 공간에 임베딩 되어야한다’
	관계는 유지하되, 특정 관계에 대해서 프로젝션 한 두 엔티티는 다르도록 학습
	
6. 그래프
	1) 가장 중요한 것: 이웃을 정의하는 방법, 그래프간의 연관성
	diffusion: 그래프 내에서 노드간 발생하는 정보의 교환
	high order proximity: 그래프 행렬 A를 제곱하면, 2 step의 연결된 가중치를 의미함. 
	n제곱 하면 n step이 됨. 이를 평균으로 나누어 평균 해주면,  high order proximity가 됨.
	이런 hop를 가지고 PPMI를 구할 수 있음.
	PPMI: 한 노드의 힘이 세졌을 때, 정보가 어떻게 전파가 되냐?
	-> 힘이 증가되면 다른 곳에서 끌어오게 되는데, 이 끌어옴에 따라 자기의 정보도 커지기도 함. 뺏긴 노드는 -가 되지만, 이를 무시하고 0으로 처리해 줌. 
	이러한 전처리 방법을 PPMI라고 함. 
	이런 식으로 연결성에 대한 정보를 나타내는 것. 이런 전처리 과정을 거친 값으로 매트릭스를 초기화하게 된다! 1,3 노드가 관련이 있다 이런 식으로.
	
7. Deepwalk & Skipgram
	: graph source에서 random하게 sequence를 생성-> 생성된 sequence로 skip-gram 학습
	
	Deepwalk 방법의 한계
	: homogeneous한 관계만을 표현할 수 있음 (다른 type의 node를 다르다고 표현하지 못함) 
	
8. metapath2vec
	:이러한 한계점을 극복하는 방법을 제시함. 다른 타입의 노드를 다르게 표현할 수 있음. 
	“대표저자”라는 관계를 표현함에 있어 같은 방향과 같은 크기로 균일하게 공간에 표현됨.
	transR 방식의 임베딩에서는 다른 타입을 다른 공간에 임베딩함으로써 이러한 차이를 표현했는데, 비슷한 접근을 취함.
	같은 공간에 맵핑 되는 것은 부적절하다는 것
	
	1) subpath 생성
	: randomwalk 기반으로 path 생성. (일반적으로 길이는 1000정도)
	
	2) neighbor window
	: window size=5를 가정, subpath에서 구한 seq에 window를 취해줌. 여기서 window 바깥의 샘플은 negative sample
	이 window 내에서, 다음에 등장할 토큰이 제대로 등장할 확률을 MLE로 극대화하도록 학습
	
	3) type별 maximum probability를 구별
	: 같은 타입(공간) 내의 확률끼리는 곱의 연산으로, 다른 공간 확률들은 합의 연산을 취해줌.
	
	4) randomwalk의 문제
	: 오직 빈도에만 의존 -> 특정 metapath 구조에 의미가 있다고 먼저 제공 (APA는 공저자, APVPA-> 같은 주제 연구 관계 등)
	
	5) path 지정
	: 시작과 끝이 동일하도록 지정, 그러나 꼭 그래야하는 것은 아님. 
	
	6) negative sampling시 type고려
	: softmax를 취해 줄 때, negative samples를 분모에 두는데 이 때 아무 negative sample만 두는 것이 아니라, 다른 타입에서 샘플링을 해 옴.
	
9. PTE 방식(텍스트 도메인)에서 차용한 방법론이라고 함.

10. metapath2vec의 성능
	1) 적은 데이터 수에서 좋은 성능을 보임.
	: 이는 유용한 metapath를 미리 지정해 주기 때문이 아닐까, 하는 추측
	
	2) 하이퍼파라미터
	: 하이퍼파라미터에 민감하지 않은 성향을 보이나, 저자의 window size에는 민감한 현상(한 저자가 한가지의 연구만 하는 것이 아니므로, 
	window size를 너무 크게 잡으면 임베딩에 방해가 됨)
	
11. 결론
	1) homogeneous grah network를 heterogeneous graph에 적용한 논문
	2) Skip-gram 구조의 변형을 통해, 다른 type 노드간 관계를 다른 공간에서 학습되도록 유도함
	3) GNN 구조에서 network가 커질 때, graph sampling 방식으로 활용도가 높음.

12. 한계점
	: 데이터 셋에 따른 최적의 metapath에 관한 선행지식이 필요
	-> 도메인 지식이 없을 경우 휴리스틱한 접근이 필요, 불확실성 존재
	