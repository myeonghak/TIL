BERT4REC-2

Sequential rec이 유용한 경우 (seasion based)
	: e-commerce 상황에서 가장 유용함
	
	자연어처리의 다양한 방법론이 응용됨
	
	GRU4REC->CASER(CNN)->SASREC(self-attention)->BERT4REC

Side Information
	: HIC(Heterogeneous Information Network)
	-> 정보 더 쓰고, explainable하게 만들자!
	KG(knowledge graph)를 만들고, 이를 임베딩하자!
	session based는 첫 로그 정보만으로도 추천을 할수 있기야 하지만, historical data를 쓴다는 점에서 한계가 있음.
	그러나 KG 기반으로는 새로운 아이템이 등장해도 이를 비슷한 임베딩 값을 갖는 상품으로 추천해 주는 등, 더 cold-start에 강건한 특성을 갖는다.
	
	최근 CNN 기반 style transfer를 활용, 유사한 이미지의 상품을 추천, fashion에서 많이 사용됨.

1.SASrec(Self-Attentive sequential rec)
	기존에는 GRU를 활용해 sequential 정보를 학습했지만, 이는 long-term dependency를 잘 반영하지 못함과 동시에 효율이 떨어지므로 트랜스포머를 활용함.
	t기까지의 seq을 활용해 t+1기의 구매 상품을 예측하는, AR(autoregressive)한 방법을 사용해 예측
	다음에 예측할 토큰을 알고 있으면 학습이 잘 이루어지지 않으므로, 행렬의 대각원소 위쪽에 위치한 삼각형의 원소들을 masking해줌.
	
	이때 relu층을 통과하여 비선형성을 더해줌. 여기서 weight를 통과 전과 후 공유해준다
	
	또, point-wise 연산이 수행됨. 이는 item간의 정보 공유는 없음을 의미하며, 1d conv net과 동일한 효과를 가짐
	
	layer norm, dropout, residual connection으로 학습 개선
	-> 여기서 dropout은 네트워크의 노드를 끊는(0으로 놓는) 것이 아니라 시퀀스 내부의 아이템을 없애버림

2.MC 방식은 어텐션을 이전 k기 시점에 몰빵하는 것으로 볼 수 있음!

3.이전의 딥러닝이 아닌 방식이 잘 작동할 수 있던 이유는 이전 시간에 크게 종속하기 때문에 그러함!