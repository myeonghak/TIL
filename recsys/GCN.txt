GCN 1

1. CNN remind 
	1) translational invariance: 사진이 옆으로 한 픽셀씩만 움직인다 하더라도 의미의 손실이 MLP 에 비해 적음
	3) CNN은 각 레이어의 activation map들의 값을 업데이트 함
	2) 적합한 convolution weight와 filter를 학습함
	4) 이 CNN을 GNN에 사용한다면?
	
2. GCN
	1) node의 feature값을 업데이트함
	: vertex를 업데이트 하는 구조도 있으나, 기본적으로 노드 업데이트
	
	2) adjacency matrix+feature matrix로 구성되어 있음. 
	3) 이 여기서 node의 feature를 업데이트 하고 싶음: 
		weight sharing+주변 노드의 정보 활용
	
	4) weight sharing: 
	activation(1번째 노드의 feature*W +2번 노드 feature*W+...+n번 노드 feature*W+b)
	-> 이 방식으로, 같은 weight인 W를 share함 
	
	5) 주변 노드의 정보 활용:
	연결된 edge의 값은 반영, 연결되지 않은 노드는 무시 (특정 노드가 다음 레이어로 업데이트 할 때)
	
	6) 업데이트 결과 Adjacency matrix는 그대로고, feature matrix의 값만 업데이트 됨
	
	7) Adjacency matrix에서, 단위행렬 I를 더해줌. 
	이는 자기 자신의 정보를 다음 레이어로 전달하기 위함(아까 연결되지 않은 node의 정보는 업데이트 하지 않음을 상기할 것)

3. matrix 연산

4. Adjacency matrix에서, 단위행렬 I를 더해줌. 
이는 자기 자신의 정보를 다음 레이어로 전달하기 위함(아까 연결되지 않은 node의 정보는 업데이트 하지 않음을 상기할 것)

5. Readout - permutation Invariance
	: 노드의 순서를 정해놓지 않을 경우, 노드의 순서가 지정됨에 따라 값의 변동이 있을 수 있다. 
	이를 해결하는 방법 중 하나로 Readout.
	node-wise summation
	: 연산의 결과로 나온 H를 MLP에 넣고, activation한 뒤 총합(sum)을 해버리자. 
	
6. GCN overall
	1) input(G(X,A)): 입력 그래프. 피처 정보 matrix X와 인접 정보 matrix A가 들어간다.
	2) graph Conv (depth=32)*3
	3) Readout(MLP)
	4) classification/regression layer
	
7. Advanced GCN
	: CNN에 응용된 다양한 기법들이 적용될 수 있음
	1) skip connection & gated skip connection (어느정도 비율로 gate를 열어줄 것인가)
	2) attention mechanism
	
8. edge 업데이트(인접행렬의 정보)가 더 중요한 경우가 있지 않나?
	-> BrainnetCNN: 인접행렬에 적합한 컨볼루션 필터를 적용