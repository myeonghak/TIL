BERT4REC-1

1. 먼저 3가지의 영역으로 추천시스템의 연구를 나눔
	1) sequential recommendation
	: 상품1-상품2-상품3 등 순차적으로 구매한 정보를 활용하는 방법, 자연어처리에서 개발된 seq 데이터 활용 방법론이 응용됨

	2) side information with embedding
	: 상품의 주변 정보, 제조국/브랜드/감독 등의 정보를 side effect라고 함. 
	여기서 heterogeneous한 정보를 추출하기 위해 graph 계열의 방법론이 활용됨. 최근 연구가 활발히 이루어지는 부분임.

	3) learning method
	: 어떻게 pos/neg 샘플을 샘플링하여 학습에 활용할지에 대한 고민.
		1] adverserial 방법으로 더욱 구분하기 어려운 샘플을 만들어 내는 방법
		2] neg sampling을 효율적으로 수행하는 방법 (주로 popular한 아이템이 사용됨) 
		두가지가 존재.

2. 일반적인 추천시스템
	1) CF기반 MF
	: sparse한 matrix를 차원축소
	-> 딥러닝 모델에서는 user-item을 latent space로 embedding, inner product를계산
	
	2) BPR(Bayesian Personal Ranking)
	: 개별 유저에 대해서 상품간의 관계를 표현하는 매트릭스를 만듦.
	
	이 때, 평가한 상품은 긍부정을 파악할 수 있지만 평가하지 않은 상품은 판단이 불가함. 
	그러나 긍정으로 평가한 상품은 적어도 평가 안한 상품보다는 낫다고 생각할 수 있음.
	
	만일 user1이 A상품을 positive하게 평가하고 B는 미평가, C는 positive일 경우, 
	A가 B상품보다 낫고, C와는 비교할 수 없고 등의 관계가 만들어짐. 
	
	이러한 representation을 가능케하는 latent vector를 찾아내는데, 
	모든 유저에 대해 likelihood를 maximize하도록 학습시키는 방법을 BPR이라 함.
	
	여기서 user간의 독립을 가정함.


3. Sequential rec이 유용한 경우 (seasion based)
	: e-commerce 상황에서 가장 유용함
	자연어처리의 다양한 방법론이 응용됨
	GRU4REC->CASER(CNN)->SASREC(self-attention)->BERT4REC

4. Side Information
	: HIC(Heterogeneous Information Network)
	-> 정보 더 쓰고, explainable하게 만들자!
	KG(knowledge graph)를 만들고, 이를 임베딩하자!
	
	session based는 첫 로그 정보만으로도 추천을 할수 있기야 하지만, historical data를 쓴다는 점에서 한계가 있음.
	
	그러나 KG 기반으로는 새로운 아이템이 등장해도 이를 비슷한 임베딩 값을 갖는 상품으로 추천해 주는 등, 더 cold-start에 강건한 특성을 갖는다.
	
	최근 CNN 기반 style transfer를 활용, 유사한 이미지의 상품을 추천, fashion에서 많이 사용됨.

5. SASrec(Self-Attentive sequential rec)
	기존에는 GRU를 활용해 sequential 정보를 학습했지만, 
	이는 long-term dependency를 잘 반영하지 못함과 동시에 효율이 떨어지므로 트랜스포머를 활용함.
	
	t기까지의 seq을 활용해 t+1기의 구매 상품을 예측하는, AR(autoregressive)한 방법을 사용해 예측
	다음에 예측할 토큰을 알고 있으면 학습이 잘 이루어지지 않으므로, 행렬의 대각원소 위쪽에 위치한 삼각형의 원소들을 masking해줌.
	
	이때 relu층을 통과하여 비선형성을 더해줌. 여기서 weight를 통과 전과 후 공유해준다.
	
	또, point-wise 연산이 수행됨. 이는 item간의 정보 공유는 없음을 의미하며, 1d conv net과 동일한 효과를 가짐
	
	layer norm, dropout, residual connection으로 학습 개선
	-> 여기서 dropout은 네트워크의 노드를 끊는(0으로 놓는) 것이 아니라 시퀀스 내부의 아이템을 없애버림

6. MC 방식은 어텐션을 이전 k기 시점에 몰빵하는 것으로 볼 수 있음!

7. 이전의 딥러닝이 아닌 방식이 잘 작동할 수 있던 이유는 이전 시간에 크게 종속하기 때문에 그러함!



8. BERT4REC
	1) 양방향으로 masked sequence를 예측함으로써 상품 그 자체의 맥락적 의미를 학습한다고 볼 수 있음
		-> bidirectional하면 안되는거 아닌가? 할 수 있으나, 예측할 때 보겠다는 것이 아니라 일반적인 의미를 파악하는 것이므로 괜찮다.
		
	2) SASRec vs BERT4Rec
		-> SASRec에서는 mask의 위치가 가장 마지막에 있다는 점, 그리고 mask가 1개라는 점만 빼고 구조 자체는 BERT4Rec과 동일한 구조
		-> 반대로 BERT4Rec은 mask가 여기저기 아무데나 뚫릴 수 있고 그 것을 예측하는 것이라는 점에서 구조적 차이가 있음
		
		-> 또 SASRec은 각 타임 step에 따른 loss를 모두 계산해줌.
		-> BERT4Rec은 Masked 단어에서 loss를 계산함.
			+ 매 epoch마다 random하게 nCk개의 sample을 masking -> data augmentation 관점에서도 장점이 존재함
			
	
	3) BERT structure - Transformer layer multi-head attention
		-> multi-head attention 방식을 차용해 왔음.
		-> dim을 multi-head 개수만큼 나누어 dimension마다 다른 정보를 학습하도록 유도(한 것 같음)
		-> attention(Q,K,V)=softmax(QK^T/sqrt(d/h))V의 결과 h*h의 attention map으로 나오게 됨.
			* 여기서는 masking 처리하지 않음 - 정보 유출 issue가 크게 문제되지 않기 때문!
		-> 이 결과를 concat하여 원래 input의 크기인 h*d가 나옴.
		-> 여기서는 Relu대신 Gelu라는 BERT에서 제안된 activation function이 사용됨.
