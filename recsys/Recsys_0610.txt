https://www.fun-coding.org/recommend_basic7.html
surprise 패키지, 실제 웹 시나리오 포함
-> 키워드: SVD prediction



https://greeksharifa.github.io/machine_learning/2019/12/21/FM/
Factorization Machine 예제 및 논문 설명



https://yamalab.tistory.com/107
[FM 모델에 대한 설명]

"여기서부터는 기본적인 Matrix Factorization 알고리즘을 잘 이해하고 있어야 Factorization Machine을 수월하게 이해할 수 있다. 
원 블로그에서는 만약 x 피처가 user와 item으로 두개의 non-zero position matrix만으로 이루어져 있다면, 
정확하게 MF와 FM이 같은 수식이라고 설명해 놓았다. 이 설명이 매우 함축적이지만 중요한 설명이다."



https://github.com/coreylynch/pyFM
pyFM

https://www.analyticsvidhya.com/blog/2018/01/factorization-machines/
xlearn 파이썬

선형 모델보다 FM이 더 성능이 좋은 이유
->
A linear or a logistic modeling technique is great and does well in a variety of problems 
but the drawback is that the model only learns the effect of all variables or features individually rather than in combination.

* 그렇다면, PCA로 만들어낸 요소로 선형 모델을 만들면 성능이 좋아지는거 아닌가? (실제로 PCA 후에 선형모델 만드는 이유가 그건가???)

* 주의점: 카테고리 변수 형태로 만들어져서, 0 1:1 2:1 3:1 형식으로 정리되어야 함. 따라서 수치형 자료(나이)는 binning 하던지, 각 값별로 dummy를 취해주던지 해야한다.


FM to the rescue
FM solves the problem of considering pairwise feature interactions. 
FM은 쌍별 피처 상호작용을 고려하는 문제를 해결해 준다.

It allows us to train, based on reliable information (latent features) from every pairwise combination of features in the model.
이는 모델 내의 모든 피처들의 쌍별 조합으로부터 신뢰할만한 정보(잠재 피처)에 기반하여 학습하도록 해준다.

FM also allows us to do this in an efficient way both in terms of time and space complexity. 
FM은 시간/공간 복잡도 관점에서도 효율적인 방법으로 이러한 작업을 수행해준다.

It models pairwise feature interactions as the dot product of low dimensional vectors(length = k). 
FM은 쌍별 피처 상호작용을 저차원 공간의 벡터들의(길이 k인) 내적으로써 모델링한다.

This is illustrated with the following equation for a degree = 2 factorization machine:
이는 다음의 degree가 2인 FM에서 다음과 같이 표현된다.

FM = w0 + wESPNxESPN + ...+ Σ<vj1,vj2>xj1xj2


Each parameter in FMs (k=3) can be described as follows:
FM(k=3)에서 각 파라미터는 다음과 같이 묘사된다.

MvESPN,vNike> = vESPN,1 * vNike,1 + vESPN,2 * vNike,2 + vESPN,3 * vNike,3

Here, for each term we have calculated the dot product of the 2 latent factors of size 3 corresponding to the 2 features. 
여기서, 2개의 피처에 대해 각각, 항마다 크기가 3인 2개의 잠재 요인의 내적을 계산한다.

모델링의 관점에서, 이는 아주 강력한데
그 이유는 비슷한 피처끼리 비슷한 공간으로 임베딩 되도록 변형되기 때문이다.
간단히 말해, 내적은 기본적으로 잠재 특성들의 유사도를 표현하고 이 유사도는 특성이 근접해 있을 때 더 높다.

"""
From a modeling perspective, 
this is powerful because each feature ends up transformed to a space where similar features are embedded near one another. 
In simple words, the dot product basically represents similarity of the hidden features 
and it is higher when the features are in the neighborhood.
"""

즉 Matrix Factorization 계열 알고리즘의 핵심 아이디어와, 
polynomial regression 계열 알고리즘의 핵심 아이디어를 결합했다고 볼 수 있다(Wide and Deep 알고리즘과 아이디어가 매우 유사하다.)



