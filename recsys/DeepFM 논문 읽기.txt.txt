Deep FM 논문 읽기

https://youtu.be/zxXRGhSQ1f4

1.MF, VAE-CF 기반 방법은 interaction만을 사용함에도 기타 feature를 활용하는 알고리즘을 out perform하는 경우가 많음.

2.그러나 이런 접근법은 한번도 구매이력이 없는 상품의 경우 추천이 이루어지지 않는다는 문제가 있음. 이를 랜덤으로 하나를 추천하는 방식으로 극복하는 방법이 실제 문제에 적용됨.

3.CTR prediction: binary prediction 모델로, 어떤 유저가 특정 상품에 노출되었을 때 클릭할 확률을 맞추는 문제. 일반적으로 interaction 외의 sparse/categorical 피처가 들어가게 되는데, 어떤 형태가 들어가는지 먼저 정의되지 않음. 이는 추천 상황이 매우 다양하기 때문. 카테고리 정보를 많이 활용하기 때문에 구매 내역이 없는 상품도 추천이 이루어짐.

4.광고 상황에서는 CTR을 미리 구해야함. pCTR이라는 지표로 단가를 미리 계산하기 때문(?), 그래서 광고의 상황에서는 interaction이 없는 데이터 상황에서 작동하는 CTR모델을 사용해야함. 

5.그러나 이를 추천에 쓸수도 있긴 함. 선호 확률을 뽑고 정렬하여 추천에 제공.

6.Field-aware Factorization Machine: 카카오 광고 팀에서도 사용. 계산이 빠르고 강력한 성능을 보임. 

7.Factorization Machine: linear model에 임의의 feature들 사이의 interaction을 additional feature로 사용.
	- 3개의 feature를 뽑은 경우 3C2의 경우를 모두 고려, 퍼블리셔/광고사/성별이 있을 때 퍼블리셔/성별 처럼 피처를 하나 생성함(onehot으로 처리하는 대신). 이 피처마다 linear model을 태운다고 함

8.FFM: FM에 하나의 가정을 더함. interaction을 Feature간만 고려하는 것이 아니라 feature와 Field의 사이에 대해서도 고려하는 것임. 아디다스(광고주)일 때 남성일 경우를 고려함.

9.Wide&Deep model
	Deep part는 임베딩 레이어와 FF 넷으로 feature를 추출함. 이를 Wide 파트에서 cross product transformation으로 결합함으로써 피처를 기억함.

10.Deep FM
	Wide and deep 모델의 cross product part에서는 바로 loss function에 넣어 logistic loss를 계산하지만, Deep FM 모델에서는 이 부분을 FM으로 바꿈. 같은 임베딩 레이어를 공유한 채로, FM 연산을 수행함.

11.CTR 모델에서는 prediction value 자체에 대한 신뢰도가 매우 중요함. 클릭할 확률값 자체가 매우 중요하다는 것. 그 이유는 그 값을 바탕으로 단가를 측정하기 때문임. 그래서 mape, ndcg 안쓰고 binary classification에서 auc/log loss를 씀.

12.Norm DNN
	: layer norm을 DNN에 적용하겠다는 아이디어, variance-only layer norm을 추가해서 성능 향상을 이뤘다고 함.

13.DeepLight
	: DeepFM 모델의 추론 속도를 성능 손실 없이 46배나 향상시킴