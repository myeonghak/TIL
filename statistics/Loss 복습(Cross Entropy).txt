
https://machinelearningmastery.com/cross-entropy-for-machine-learning/


A Gentle Introduction to Cross-Entropy for Machine Learning


크로스 엔트로피는 정보 이론에서 사용되는 개념으로, 두 확률 분포의 차이를 계산한다.
KL divergence와 밀접히 연관되어있지만 다름. KLD는 두 확률 분포 사이의 상대적 엔트로피를 계산하고, 
cross-entropy는 두 분포 사이의 전체 엔트로피를 계산한다는 점에서 다르다.
이 둘은 다르지만, 크로스 엔트로피는 KLD를 사용해서 계산될 수 있다.

크로스엔트로피는 logistic loss(로그 로스)와 관련이 있으며 또 종종 혼동되기도 함. 크로스 엔트로피와 로그로스는 다른 소스에서 유도되었지만,
분류 모델의 손실함수로 사용될 경우에는 같은 값으로 계산되며 또 서로 교차해서 사용할 수 있다.




[크로스 엔트로피란 무엇인가?]
크로스엔트로피는 주어진 랜덤 변수 혹은 일련의 사건에 대한 두 확률 분포 간의 차이의 측정이다.
정보란, encode하기 위해 필요한 bit의 수를 의미한다. 
낮은 확률로 등장하는 사건의 경우, 더 많은 정보를 가지고 있고 높은 확률로 등장하는 사건의 경우 더 적은 정보를 가지고 있다.
가령 어떤 사람이 미국에서 왔다는 이야기는 룩셈부르크에서 왔다는 이야기보다 더욱 정보가 적은 것과 같다.

엔트로피란, 확률 분포로부터 무작위로 선택된 한 사건을 전송하기 위해 필요한 비트의 수를 의미한다. 
편향된 분포는 적은 엔트로피를 가지고, 균일하게 확률 분포된 사건은 더 큰 엔트로피를 가진다.
(양궁 대회 100개의 상 중 90%가 한국인이 차지하고, 나머지를 타 국가가 나누어 가지는 경우는 이 경우를 비트로 표현하기 위해 많은 비트가 필요하지 않음.
반대로 100개의 국가가 1개씩 상을 가져간다면, 이 모든 경우의 수를 일일이 비트로 표현해 주어야 하기 때문에 엔트로피가 높다고 할 수 있다.)

정보이론에서는, 사건의 "놀라움"을 묘사한다. 작은 확률로 발생하는 사건은 더 많은 정보를 가지고 있다. 
반면에 사건들이 똑같이 개연성 있는 확률 분포의 경우 더 많은 "놀라움"이 있을 것이고, 따라서 더 큰 엔트로피를 갖는다.

- 편향된 확률 분포(안 놀라움) : 적은 엔트로피
- 균등한 확률 분포(놀라움) : 큰 엔트로피


엔트로피는 X 이산 상태 집합의 확률변수 x와 그의 확률 P(x)에 대해 다음과 같이 계산된다.
H(x) = -sum x in X P(x) * log(P(x))


크로스 엔트로피는 정보 이론의 엔트로피의 아이디어에서 출발해서, 다른 분포와 비교해 봤을 때 한 분포에서의 평균적 사건을 표현/전송할 때 필요한 비트의 수를 계산한다.
"...크로스 엔트로피는 모델 q를 사용했을 때 분포 p의 출처에서 나오는 데이터를 encode하기 위해 필요한 비트의 평균적인 수를 의미한다..."

Cross-entropy builds upon the idea of entropy from information theory and 
calculates the number of bits required to represent or transmit an average event 
from one distribution compared to another distribution.

… the cross entropy is the average number of bits needed to encode data 
coming from a source with distribution p when we use model q …

이러한 정의를 위한 직관은 다음과 같다.

만일 우리가 목표(혹은 잠재된) 확률 분포 P와 이 목표 분포의 근사인 Q를 고려할 때, 
P에서 Q로의 크로스 엔트로피는 P 대신 Q를 사용해 사건을 표현할 때 추가적으로 필요한 bit의 수를 의미한다

The intuition for this definition comes 
if we consider a target or underlying probability distribution P and an approximation of the target distribution Q, 
then the cross-entropy of Q from P is the number of additional bits to represent an event using Q instead of P.


H(P, Q) = – sum x in X P(x) * log(Q(x))

여기서 P(x)는 분포 P 상에서의 x 사건의 확률을 의미하고, Q(x)는 Q 분포 상에서 사건 x의 확률을 의미한다. 
log는 밑이 2인 로그를 의미하는데, 결과가 bit라는 것을 의미한다.
(만약 밑이 자연상수 e가 된다면, 그 결과는 nats라는 단위를 가지게 될 것이다.) -> bits 말고 nats




[케라스 크로스 엔트로피 계산 방법]

크로스엔트로피를 각 샘플별로 계산한 후 평균침.


비교 1)

import tensorflow as tf
y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
# Using 'auto'/'sum_over_batch_size' reduction type.
cce = tf.keras.losses.CategoricalCrossentropy()
cce(y_true, y_pred).numpy()

-> 1.1769392

비교 2)

a = tf.constant([0., 1., 0., 0., 0., 1.], shape=[2,3])
print(a)
b = tf.constant([.05, .95, 0., .1, .8, .1], shape=[2,3])
print(b)
cce = tf.keras.losses.CategoricalCrossentropy()
cce(a, b).numpy()

-> [0.05129331, 2.3025851]
-> np.mean([0.05129331, 2.3025851])

= 1.1769392



[크로스 엔트로피와 KLD]
크로스 엔트로피는 KLD가 아니다.

크로스 엔트로피는 Divergence measure와 관련되어 있다. 예를 들어 KLD는 한 분포가 다른 분포와 얼마나 다른지를 정량화한다.

특히, KLD는 크로스 엔트로피와 대단히 비슷한 양을 계산 값으로 갖는다. 
참 분포 P를 근사 분포 Q로 표현할 경우, 추가적으로 필요한 bit의 평균 수를 계산한다. (전체 bit의 수를 계산하는 것이 아니라)

"다른 말로,KLD는 참 분포인 P 대신 Q라는 분포를 사용함으로써 
데이터를 encode하기 위해 추가적으로 필요한 bit의 평균 수를 의미한다." - 58p, Machine Learning : A Probabilistic Perspective, 2012

이처럼, KLD는 종종 "상대적 엔트로피"로 일컫어진다.

- 크로스 엔트로피 : P 대신 Q 분포로부터 한 사건을 표현하기 위한 전체 비트의 평균 수
- 상대적 엔트로피 (KL Divergence): P 대신 Q 분포로부터 한 사건을 표현하기 위해 필요한 추가적인 비트의 평균 수

KLD는 (P의 각 사건의 확률의 음수 합) * log(Q(x) / P(x)) 로 구해진다. 일반적으로 log는 2를 밑으로 쓴다. 이는 결과값이 bit로 구해지게 하기 위해서이다.

KL(P || Q) = – sum x in X P(x) * log(Q(x) / P(x))

따라서, 크로스 엔트로피는 KLD에 P의 엔트로피를 더해줌으로써 구할 수 있다. 이는 각 계산 방식의 정의에 따랐을 때 매우 직관적이다.

H(P, Q) = H(P) + KL(P || Q)




* KLD는 참분포와 근사분포 사이의 거리라고 직관적으로 이해할 수는 있다.
그러나 이는 엄밀히 말해서는 거리가 아니다. 왜냐하면 KL(P||Q) != KL(Q||P) 이기 때문 (이 외에도 많은 이유가 있다)

편의를 위해 KLD는 참분포와 근사분포 사이의 거리라고 생각해보자.
P 분포를 따르는 사건들을 표현하기 위해 필요한 엔트로피 값인 H(P)에, P가 아니라 어쩔수 없이 Q를 사용함으로써 발생하는 추가적인 엔트로피를 KL(P||Q)를 더하면
우리가 P 대신 Q를 사용할 때 필요한 전체 엔트로피 값을 구할 수 있고,
이 전체 엔트로피 값을 줄여나가는 방식으로 학습 파라미터를 업데이트 한다면 우리는 우리가 원하는 모델을 만들어낼 수 있다.

-> 따라서 loss function의 관점에서, KLD와 CE는 동일한 기능을 수행한다!





