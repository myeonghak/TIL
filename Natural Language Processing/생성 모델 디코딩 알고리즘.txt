[생성 모델의 디코딩]

https://blog.pingpong.us/generation-model/


**** Deterministic Methods (결정론적 방법) ****

* Beam Search
https://ratsgo.github.io/deep%20learning/2017/06/26/beamsearch/
Beam Search란 최고우선탐색(Best-First Search) 기법을 기본으로 하되 기억해야 하는 노드 수를 제한해 효율성을 높인 방식입니다. 
https://velog.io/@nawnoes/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-Beam-Search
더 쉬운 설명
-> 전체 중 가장 높은 확률을 뽑아내는 그리디 서치를 보완하는 방법임.
-> 빔의 수가 1인 빔서치는 그리디 서치와 같음.
-> 빔의 수는 5-10 정도를 사용하고, 빔이 클수록 타겟 시퀀스가 맞을 확률이 높아지지만 디코딩 속도가 떨어짐.

주어진 확률 시퀀스와 빔 크기 k에 대해 빔 탐색을 수행하는 함수를 작성한다.

각 후보 시퀀스는 가능한한 모든 다음 스텝들에 대해 확장된다.
각 후보는 확률을 곱함으로써 점수가 매겨진다.
가장 확률이 높은 k 시퀀스가 선택되고, 다른 모든 후보들은 제거된다.
위 절차들을 시퀀스가 끝날때까지 반복한다.

https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/



**** Stochastic Methods (확률론적 방법) ****
확률론적 방법은 각 스텝마다 확률 분포에 따른 샘플링으로 다음 단어가 결정되는 방법을 말합니다. 
결정론적 방법과 다르게 매번 샘플링 결과가 달라지기 때문에 상대적으로 더 다양한 문장이 만들어집니다. 대표적인 알고리즘들은 다음과 같습니다.

Direct Sampling: 
Decoder의 출력인 logit에 직접 softmax를 취한 뒤, 이를 categorical distribution으로 보고 하나의 단어를 샘플링하는 방식입니다. 
이 접근은 beam search에 비해 다양한 응답을 생성한다는 장점이 있지만,
현재 스텝에서 매우 낮은 점수를 부여받은 단어들(확률 분포상 Long tail에 속하는 단어들)도 샘플링 대상에 포함되기 때문에, 문법적으로 맞지 않거나 상대적으로 어색한 문장을 만들어낸다는 문제점이 있습니다.

Temperature Sampling [14]: 
각 단어의 점수를 0부터 1 사이의 상수 t(temperature) 로 나눠준 뒤 softmax를 취하고, 여기서 샘플링을 하는 방법입니다. 

"""Temperature 값이 0에 가까워질수록 낮은 확률을 부여받았던 단어의 확률은 더욱 낮아지고 높은 확률을 부여받은 단어의 확률이 높아지기 때문에"""

, Direct Sampling이 가진 문제를 어느 정도 완화합니다.

Top-k Sampling: 
Top-k 알고리즘은 모든 단어가 아니라 가장 높은 확률을 보인 상위 k개의 단어만을 대상으로 샘플링을 진행하는 방법입니다. 
이를 통해 문법 오류를 일으킬 수 있는 낮은 확률의 단어들을 샘플링 대상에서 제외하면서도, 샘플링 기반 접근의 장점을 기대할 수 있습니다.

Nucleus Sampling [13]: 
이 알고리즘은 먼저 한 스텝에서 계산된 모든 단어의 확률을 내림차순으로 정렬한 뒤 선택된 단어들의 확률의 누적합이 미리 정해둔 값 P가 될 때까지 차례로 단어 후보군을 선택하는 방식입니다. 
이 방식에서는 이후 선택된 단어들의 확률을 normalize한 뒤 일반적인 categorical distribution처럼 샘플링합니다. 
Long tail에 해당하는 단어들을 샘플링 대상에서 제외한다는 점에서 Top-k와 유사한 알고리즘으로 볼 수 있습니다. 
원 논문에 의하면 Top-k sampling에 비해 샘플링 대상에 포함되는 단어의 개수가 문맥에 따라 유동적으로 변하는 장점이 있다고 합니다.