[DeepWalk 논문 번역]

나의 요약
: 그래프 노드의 사회적 관계를 함축한 잠재표현을 학습하기 위해, 언어 모델의 skip-gram 방식을 차용함.
무작위로 그래프 내를 순회하며, 자주 맺어지는 
- 비슷한 위치에 등장하는 단어는 비슷한 의미를 갖는다는 가정을 사용?




요약
네트워크의 꼭지점들의 잠재 표현을 학습하기 위한 새로운 접근을 제시함. 이 접근법의 이름은 DeepWalk(이하 딥워크)임.
이 잠재 표현은 통계적 모델에 의해 쉽게 활용될 수 있는 연속적인 벡터 공간 내의 사회적 관계를 인코딩함.
딥워크는 언어 모델링과 비지도적 피처 학습 (혹은 딥러닝)의 최근 발전을 단어 시퀀스로부터 그래프로 일반화함.
딥워크는 잠재 표현을 학습하기 위해 walk를 문장과 동일한 것으로 다룸으로써 truncate된 random walk에서 획득한 지역적 정보를 사용함.
BlogCatalog, Flickr, YouTube와 같은 소셜 네트워크를 위한 멀티 라벨 네트워크 분류 task에 딥워크의 잠재 표현 표현을 검증함.

결과적으로, 딥워크는 네트워크의 전역적 관점을 활용한 다른 경쟁 베이스라인 모델들을 성능상 앞섰는데, 특히 결측 정보가 존재했을 때 두드러짐.
딥워크의 representation은 라벨 데이터가 sparse할 때 경쟁 방법론에 비해 f1 score를 최고 10%까지 앞섰음.
몇 실험에서, 딥워크의 representation은 모든 베이스라인 방법론을 60% 더 적은 학습 데이터를 사용하면서도 성능상 앞섰음.
딥워크는 또한 확장 가능함(scalable).
온라인 학습 알고리즘으로, 유용한 증가하는 결과를 쌓고, 또한 매우 간단하게 병렬화할 수 있다.
이러한 특징들은 딥워크를 네트워크 분류, 이상치 탐지 등 현실의 넓은 응용 분야에 활용할 수 있게함.


결론
네트워크 꼭지점의 잠재적 사회적 표현을 학습하는 새로운 접근인 딥워크를 제안함.
truncated된 random walks로부터 얻은 지역적 정보를 입력값으로 사용해, 우리의 방법론은 구조적 패턴을 인코딩하는 표현을 학습함.
다양한 종류의 그래프에 수행한 실험은 멀티 라벨 분류 task에 대한 딥워크 접근의 효율성을 보여줌.
온라인 알고리즘으로써, 딥워크는 확장 가능함. 실험 결과 spectral한 방법을 사용하기에 지나치게 큰 그래프의 유의미한 표현을 생성하는 데 사용할 수 있다는 것을 보여주었음.
이러한 큰 그래프에는, 우리의 방법론이 sparsity를 다루기 위해 고안된 다른 방법론을 유의미하게 성능상 앞섰음.
또한 이 접근은 병렬화 가능하며, 모델의 다른 부분을 동시에 업데이트 할 수 있도록 함.
효율성과 확장 가능성에 더해, 이 접근은 언어 모델의 설득력 있는 일반화임.
이 연결 관계는 상호적으로 이로움. 언어 모델링의 발달은 네트워크의 개선된 representation을 생성하는 것으로 이어질 수 있음.
우리의 관점에서, 언어 모델링은 사실상 관측 불가능한 언어 그래프에서 샘플링을 하는 것임.
우리는 관측 가능한 그래프를 모델링함으로써 얻어낸 통찰은 결과적으로 관측 불가능한 그래프(언어 그래프)의 개선을 이끌어낼 수 있다고 믿음.
우리의 해당 분야에서 미래 작업은 이 이중성을 더욱 탐구하는 데 집중할 것이며, 우리의 결과를 언어 모델링을 개선하는 데 사용하고, 방법론의 이론적 정당화를 강화할 것임.


1. 도입
네트워크 표현의 sparsity는 약점인 동시에 강점임.
sparsity는 효율적인 이산적 알고리즘의 설계를 가능하게 하지만, 통계 모델로 일반화하기 어렵게 할 수 있음.
네트워크에서의 ML 활용(네트워크 분류, 컨텐츠 추천, 이상치 탐지, 결측 링크 예측 등)은 생존하기 위해 이러한 sparsity를 다룰 수 있어야 함.
이 논문에서 네트워크 분석에서 딥러닝(비지도적 feature 학습) 기법을 처음으로 도입함. 이는 자연어 처리에서 성공적인 결과를 보여 준 방법임.

우리는 짧은 random walk 흐름을 모델링 함으로써 그래프 꼭지점의 social representation을 학습하는 딥워크라는 알고리즘을 개발했음.
social representation은 꼭지점들의 이웃간 유사도와 집단 멤버십을 잡아내는 잠재 특성임.
이러한 잠재 표현은 상대적으로 낮은 차원의 연속적인 벡터 공간 내에 사회적 관계를 인코딩함.
딥워크는 자연어 모델을 일반화하여, 무작위적으로 생성한 walk의 집합으로 구성된 특수한 언어를 처리함.
이러한 자연어 모델은 인간 언어의 의미적이고 합성적인 구조를 잡아내는 데 사용되며, 논리적인 유추 역시 포착할 수 있음.


딥워크는 그래프를 입력값으로 받아 잠재 표현을 아웃풋으로 출력함.
우리의 방법론을 잘 연구된 karate 네트워크에 적용한 결과는 그림 1과 같음.
힘의 방향이 표시된 구조로 흔히 표현되는 그래프는, 그림 1a에 보여짐. 그림 1b는 2차원 잠재 공간에서의 우리 방법론의 결과값을 보여줌.
현격한 유사도를 떠나서, (1b)의 선형 분리 가능한 부분들이 입력 그래프 (1a)에서 modularity maximization으로 찾아진 클러스터에 상응한다는 사실에 주목했음.

실세계 시나리오에서의 딥워크의 잠재력을 증명하기 위해,
거대한 이질적 그래프에서의 고난도의 멀티 라벨 네트워크 분류 문제에 대해 성능을 평가했음.
관계성 분류 문제에서, feature 벡터간의 링크는 전통적인 i.i.d. 가정을 위반함.
이러한 문제를 다루기 위한 기법들은 전형적으로 분류 결과를 개선하기 위해 의존성 정보를 활용하는 근사적 추론 기법(approximate inference techniques)을 사용함. 
우리는 그래프의 라벨 독립적인 표현을 학습함으로써 이러한 접근으로부터 거리를 두었음.
우리의 representation의 품질은 라벨링된 꼭지점의 선택에 의해 영향을 받지 않으며, 따라서 다양한 task들 간에 공유될 수 있음.
딥워크는 social dimension을 생성하기 위한 다른 잠재 표현 방법론을 성능상 개선했는데, 특히 라벨링된 노드가 부족할 때 뛰어났음.

우리의 representaion의 강력한 성능은 아주 간단한 선형 분류기(예를 들어 로지스틱 회귀)로도 성취 가능함.
우리의 표현은 일반적이고, 따라서 어떤 분류 방법 (반복적 추론 방법론, iterative inference method)이든간에 결합될 수 있음.
딥워크는 아주 간단하게 병렬화 가능한 온라인 알고리즘이면서도 동시에 이 모든 것을 이뤄냈음.

우리의 contribution은 다음과 같음:
- 딥러닝을 도입해 그래프를 분석하고, 강건한 representation을 구축해 통계 모델링에 적합하도록 만들었음.
	딥워크는 짧은 랜덤 워크 간에 존재하는 구조적 유사성을 학습함.
- 우리의 representation의 성능을 여러 social network의 멀티 라벨 분류 task에 포괄적으로 검증함.
	라벨의 sparsity가 있을 때 분류 성능의 유의미한 증가를 보였고, 우리가 고려한 가장 sparse한 문제에서 5~10%의 micro f1 성능 개선을 얻었음. 
	몇 몇 경우에서는, 딥워크의 representation은 60%나 더 적은 데이터가 주어진 경우에도 경쟁 알고리즘을 앞섰음.
- 병렬 처리를 사용해 web-scale 그래프(유튜브와 같은)의 representation을 만들어 냄으로써 우리 알고리즘의 확장가능성을 증명함. 
	나아가, 우리 접근의 시행중인 버전을 구축하기 위해 필요한 최소한의 변경을 기술하였음.
	
해당 논문의 나머지 부분은 다음과 같이 정리됨.
2,3절에서는 데이터 네트워크에서 분류 분제의 형성을 논의하고, 이 것이 어떻게 우리의 작업과 관련돼 있는지를 보임.
4절에서는 social representation 학습을 위한 우리의 접근인 딥워크를 제시함.
실험의 큰 그림을 5절에서 보이고, 결과를 6절에서 제시함.
유관 연구에 대한 논의를 7절에서 보여주고, 결론을 제시한 뒤 맺음.


2. 문제 정의
우리는 하나 혹은 그 이상의 카테고리로 소셜 네트워크의 멤버를 분류하는 문제를 고려함.
좀 더 형식적으로는, G=(V,E)일때 V는 네트워크의 멤버들을 뜻하고, E는 그의 간선들을 의미함. E ⊆ (V × V ).

전통적인 머신러닝 분류 상황에서는, X의 원소들을 라벨 셋 y에 맵핑하는 가설 H를 학습하는 것이 목적임.
우리의 경우에는, G의 구조 내에 임베딩된, 예시들의 의존성에 대한 유의미한 정보를 사용해 개선된 성능을 얻을 수 있음.

관련 문헌에서는, 이는 관계적 분류(relational classification)이라고 알려져 있음. (혹은 연속적 분류 문제, collective classification problem)
관계적 분류에 대한 전통적인 접근은 무방향 마르코프 체인에서의 추론으로써 문제를 제시하고, 반복적인 근사 추론 알고리즘을 사용해(예를 들면 iterative clasification 알고리즘, 깁스 샘플링, 라벨 완화 label relaxation) 
네트워크 구조가 주어졌을 때 라벨의 사후 분포를 계산함.

네트워크의 위상적 정보를 잡아내기 위해 다른 접근을 제안함.
라벨 공간을 feature 공간의 일부로 혼합하는 것 대신에, 라벨의 분포에 독립적인 그래프 구조를 잡아내는 feature를 학습하는, 비지도적 방법을 제안함.

이러한 구조적 representation과 라벨링 task간의 분리는 반복적 방법론에서 발생할 수 있는 cascading(계단식) 에러를 회피함. 
나아가, 같은 representation은 해당 네트워크레 관련된 복수의 분류 문제에 사용될 수 있음.

우리의 목표는 저차원 잠재 공간 내에서의 X의 임베딩을 학습하는 것임. 
이러한 저차원 representation은 분포되어 있음. 
즉, 각 사회 현상은 차원의 부분집합으로 표현되며 각 차원은 공간에 의해 표현되는 사회적 개념의 부분집합을 이루는 것.

이러한 구조적 feature를 사용해, attribute 공간을 증강하여 분류 결정을 도울 것임.
이러한 feature들은 범용적이고, 어느 분류 알고리즘(반복적 방법 포함)과도 함께 사용될 수 있음.
그러나, 이러한 feature의 가장 큰 효용은 단순한 머신러닝 알고리즘과의 손쉬운 통합임.
실 세계 네트워크에 적절히 확장됨. 이는 6절에서 보여줄 것임.



3. social representation 학습
다음 특징을 가진 social representation을 학습하려고 함:
- 적응성: 실제 소셜 네트워크는 항상 진화함, 새로운 사회적 관계는 전체 학습 과정을 모두 재반복할 것을 요구해서는 안됨.
- 집단에 대한 인지: 잠재 차원간의 거리는 네트워크의 상응하는 멤버들 간의 사회적 유사도를 평가하는 측도를 표현해야 함. 
	이는 동종 선호적 네트워크 안에서 일반화를 가능케 함.
- 저차원성: 라벨 데이터가 부족할 때, 저차원 모델은 일반화를 더 잘 수행하며, 수렴과 추론이 더 빠름.
- 연속성: 연속된 공간 내의 부분적인 집단 멤버십을 모델링 하기 위해 잠재 표현이 필요함. 
	집단 멤버십에 미세한 기질이 더해진 관점을 제공할 뿐만 아니라, 연속적인 표현은 더 부드러운 집단간의 결정 경계선을 가짐. 이는 더 강건한 분류를 가능하게 함.

이러한 조건들을 만족하기 위한 우리의 방법론은 꼭지점에 대한 표현을 한 흐름의 짧은 랜덤워크에서 학습하며,
본래 언어 모델링을 위해 고안된 최적화 기법을 사용함.
여기서, 기본적인 랜덤워크와 언어 모델링을 복습하며, 어떻게 이들의 조합이 우리의 요구사항을 만족하는지 기술할 것.

	3.1 랜덤 워크 (Random Walks)
	
	꼭지점 vi에 뿌리를 둔 랜덤워크를 W_vi로 표시함.
	이는 W1_vi, W2_vi, . . . , Wk_vi라는 무작위 변수를 사용하는, Wk+1_vi이 꼭지점 vk의 이웃에서부터 무작위로 선택된 꼭지점인 확률적 절차임.
	랜덤워크는 컨텐츠 추천과 집단 감지와 같은 다양한 문제에서 유사도 측도로 사용된 방법론임.
	랜덤워크를 입력 그래프의 사이즈에 아선형적인 시간 구조를 갖는 지역적 집단 구조 정보를 계산하기 위해 사용하는 출력 민감 알고리즘(output sensitive algorithms)계열의 근간이기도 함.

	바로 이 지역적 구조에 대한 연결점이 바로 네트워크에서 정보를 추출하기 위한 도구로써 일련의 짧은 랜덤워크를 사용하는 포인트임.
	지역적 정보를 포착하는 데서 나아가, 랜덤 워크를 우리의 알고리즘의 근간으로 사용하는 것은 다른 바람직한 특성을 제공함.

	첫째로, 지역적 탐색이 쉽게 병렬화됨.
	복수의 랜덤 워커 (다른 쓰레드, 프로세스, 혹은 머신에서)들이 동시에 같은 그래프의 다른 부분을 탐색할 수 있음.
	
	두번째로, 단기간의 랜덤 워크에서 획득한 정보에 의존하는 것은 그래프 구조 내의 작은 변화를 전역적 재연산의 필요 없이 수용할 수 있게 함.
	우리는 학습된 모델을 변경된 지역에서 새로운 랜덤워크로 반복적으로 업데이트할 수 있음(전체 그래프 대상, 아선형 시간 이내로). 

	3.2 연결: 멱법칙
	그래프 구조를 파악하기 위한 기초로써 온라인 랜덤 워크를 선택했기 때문에, 이 정보를 파악하기 위한 적절한 방법론이 필요함.
	연결된 그래프의 차수(degree) 분포가 power law(무척도인)를 따른다면, 짧은 랜덤 워크에서 등장하는 꼭지점의 빈도는 멱법칙을 따른다는 것을 관찰함.

	자연어의 단어 빈도는 비슷한 분포를 따르고, 또한 자연어 모델링에서 사용되는 기법이 이 분포의 행동을 처리함.
	이 유사도를 강조하기 위해, 두 다른 멱법칙을 그림 2에 보임.
	첫번째 그림은 무척도의(scale-free) 그래프 내 일련의 짧은 랜덤 워크에서 온 것이고, 두번째 그림은 영어 위키피디아의 10만 게시글의 텍스트에서 온 것임.
	우리의 연구의 핵심 공헌은, 자연어 처리 모델링(기호 빈도가 지프의 법칙, 멱법칙을 따르는)에서 사용되어 온 기법들이 네트워크 내의 집단 구조를 모델링하는데 재사용될 수 있다는 것임.
	우리는 나머지 부분을 자연어 처리의 발달 중인 연구를 검토하고, 이를 우리의 기준을 만족시키는 곡지점의 표현을 학습하기 위해 변형함.


	3.3 언어 모델링
	언어 모델링의 목표는 코퍼스 내에 특정 단어 시퀀스가 등장할 확률을 추정하는 것임.
	더 공식적으로는, 단어 시퀀스 Wn_1 = (w0, w1, · · · , wn) where wi ∈ V (V 는 단어사전) 가 주어졌을 때,
	모든 학습 코퍼스에 걸쳐 확률 Pr(wn|w0, w1, · · · , wn−1)를 극대화하도록 학습함.

	최근의 표현학습(representation learning)에 관한 연구는 언어 모델링의 범위를 원래 목적 너머로 확장시킨 
	확률적 신경망을 사용해 단어의 일반화된 표현을 찾는 데 주안점을 두고 있음.

	이 연구에서, 우리는 언어 모델링의 일반화를, 일련의 짧은 랜덤워크를 통해 그래프를 탐색함으로써 보임.
	이러한 랜덤워크는 특수한 언어의 짧은 문장과 구문으로 생각될 수 있음.
	직접적인 비유는, 모든 이전의 랜덤워크로 방문된 꼭지점이 주어졌을 때 꼭지점 vi를 관찰하는 우도를 추정하는 것.
	Pr(vi | (v1, v2, · · · , vi−1))

	우리의 목표는 노드 동시 발생의 확률 분포 뿐만아니라 잠재 표현을 학습하는 것임. 따라서 맵핑 함수 Φ: v ∈ V (|V|*d 차원 실수 공간 내)를 고안함. 
	이 맵핑함수 Φ는 그래프 내 각 꼭지점 v와 연관된 잠재적 social representation을 나타냄.
	(실제로는, Φ를 자유 파라미터의 |V|*d 행렬로 나타내었는데, 이는 추후에 X_E로 활용됨)

	그렇다면 문제는, 다음의 우도를 추정하는 문제가 됨.
	Pr (vi |Φ(v1), Φ(v2), · · · , Φ(vi−1)) (1)

	그러나 워크의 길이가 증가함에 따라, 이 목적함수를 계산하는 것은 실행 불가능해짐. 최근 언어 모델링의 완화는 이 예측 문제를 완전히 뒤집어 생각함.
	먼저, 결측 단어를 예측하기 위해 맥락을 사용하는 것이 아니라, 한 단어를 사용해 맥락을 예측함.
	둘째로, 맥락은 오른쪽 혹은 왼쪽 편에 등장하는 단어들로 구성되어 있음.
	마지막으로, 이는 문제의 순서 제약을 제거함.

	대신, 모델은 주어진 단어로부터 일어나는 상쇄에 대한 지식 없이 맥락 안에 등장하는 어떠한 단어의 확률이든 극대화하도록 요구받고, 
	꼭지점 표현 모델의 관점에서, 이는 최적화 문제를 만들어낸다.
	minimize Φ − log Pr({vi−w, · · · , vi−1, vi+1, · · · , vi+w} | Φ(vi)) (2)

	우리는 이러한 완화가 social representation 학습에 특히 바람직하다는 것을 발견했음.
	먼저, 순서 독립 가정은 랜덤워크에 의해 제공받는 '근접성'의 감각을 더 잘 잡아냄.
	더욱이, 이 완화는 한 꼭지점이 한 개씩 주어짐에 따라 작은 모델들을 만들어
	학습 시간을 가속하는 데 상당히 유용함.

	이 2번 식의 최적화 문제를 풀어내는 것은 꼭지점 간의 지역적 그래프 구조 안의 공유된 유사성을 잡아내는 표현을 만들어 냄.
	유사한 이웃들을 가진 꼭지점들은 비슷한 표현을 얻어낼 것이고(동시 인용 유사도를 인코딩하며),
	머신러닝 task에 일반화를 가능케 할 것임.

	truncated 랜덤워크와 신경망 언어 모델을 결합함으로써, 우리가 희망했던 특성을 만족시키는 방법론을 수식화 시켰음.
	이 방법론은 연속된 벡터 공간 내 저차원에서의 소셜 네트워크의 표현을 생성해 냄.
	이 표현은 집단 멤버십의 잠재적인 형태를 인코딩하고, 이 방법론이 유용한 중간 표현을 뱉어내기 때문에, 변화하는 네트워크 위상 구조에도 적응할 수 있음.



4. 방법론
이 섹션에서는, 우리의 알고리즘의 주된 부분을 논의할 것임.
우리 접근의 몇 몇 변형을 보여주고, 그들의 장점에 대해 설명할 것.

	4.1 개요
	여느 언어 모델링 알고리즘과 같이, 필요한 입려값은 코퍼스와 단어사전 V임.
	딥워크는 한 짧은 truncated 랜덤워크 집합을 코퍼스로 간주하고, 그래프 꼭지점을 단어사전으로 간주함.
	학습 전에 랜덤워크 내의 꼭지점의 빈도 분포와 그래프 꼭지점의 단어사전을 아는 것은 유익하지만, 알고리즘이 작동하기 위해 꼭 필요한 것은 아님.

	4.2 알고리즘: 딥워크
	이 알고리즘은 두 핵심 부분으로 구성되어 있음.
	첫번째는 랜덤워크 생성기이고, 두번째는 업데이트 절차임.
	랜덤워크 생성기는 그래프 G에서, 균일 분포로 랜덤 꼭지점 vi를 랜덤워크 W_vi의 루트로써 샘플링함.
	한 워크는 마지막 방문된 꼭지점의 주변 이웃으로부터 균일하게 샘플링해, 최대 길이 t를 도달할 때까지 반복함.
	우리의 랜덤워크의 길이를 고정되게 설정하였지만, 랜덤워크가 같은 길이로 설정되어야한다는 제한은 없다.

	이러한 워크들은 재시작(즉, 원래 루트로 재이동할 순간이동 확률)을 가질 수 있지만, 예비 실험 결과로는 재시작의 이점이 드러나지 않았음.

	알고리즘 1의 3~9번 줄이 우리 접근의 핵심적인 아이디어를 나타냄.
	외부 루프는 각 꼭지점에서 랜덤워크를 시작할 횟수 γ를 명시함.
	각 반복을 데이터에 걸쳐 '통과'를 만들고, 한 워크를 노드별로 이 통과마다 샘플링한 것으로 생각할 수 있음.
	각 통과의 시작마다, 각 꼭지점을 순회할 무작위적 순서를 생성함.
	이는 엄격하게 요구되는 절차는 아니지만, SGD 과정의 수렴을 가속한다고 알려져 있음.

		4.2.1 SkipGram
		스킵 그램은 문장 내 윈도우 사이에 등장하는 단어들 간의 동시 등장 확률을 극대화하는 언어모델임. 알고리즘 2는 윈도우 w사이에 등장하는 랜덤워크에서, 모든 가능한 연어(연속 어휘)에 걸쳐 반복됨.
		각 꼭지점 v_j를 현재의 d차원 representation 벡터 Φ(v_j)에 각각 맵핑함.

		v_j의 representation이 주어졌을 때, 워크 내의 이웃들의 확률을 극대화하고자 함. 이러한 사후 확률을 분류기의 여러 선택을 통해 학습함.
		예를 들어, 이전 문제를 로지스틱 회귀를 사용해 모델링하는 것은 |V|와 동일한, 수백만/수십억에 이를 수 있는 매우 많은 수의 라벨이 나올 것임.
		이런 모델들은 클러스터 단위의 컴퓨터에까지 이를 수 있는 막대한 컴퓨팅 자원을 요구함.
		학습 속도를 빠르게 하게 위해, 계층적 소프트맥스(Hierarchical Softmax)가 확률 분포를 근사하기 위해 사용될 수 있음.


		4.2.2 계층적 소프트맥스
		u_k ∈ V가 주어졌을 때(여기서 u_k는 주어진 랜덤워크 sequence 내의 root 노드 이외의 노드),  Pr(u_k|Φ(v_j))를 계산하는 것은 실현 불가능함. 
		분할 함수(partition function) (정규화 요소)를 계산하는 것은 비용이 많이 듦.
		만약 꼭지점을 이진 트리의 잎에 할당한다면, 예측 문제는 트리 내의 특정 경로의 확률을 극대화하는 문제로 바뀜.
		만일 꼭지점 u_k로 이르는 경로가 트리 노드의 시퀀스로 규명된다면, v_j의 representation 벡터 Φ(v_j)가 주어졌을 때 u_k의 확률을
		v_j의 representation 벡터 Φ(v_j)가 주어졌을 때 트리 노드의 시퀀스 b_l이 등장할 확률의 product로써 표현할 수 있음.
		이제, v_j의 representation 벡터 Φ(v_j)가 주어졌을 때 트리노드 시퀀스 b_l의 확률은 노드 b_l의 부모 노드에 할당되는 이진 분류기에 의해 모델링될 수 있음.
		이 작업을 통해 Pr(u_k|Φ(v_j))의 계산 복잡도를 O(|V|)에서 O(log|V|)로 줄일 수 있음.
		빈번히 등장하는 꼭지점에 더 짧은 경로를 할당함으로써 학습 과정을 더욱 빠르게 만들 수 있음. 허프만 코딩이 트리 내의 빈출 원소의 접근 시간을 줄이는 데 사용됨.

		4.2.3 최적화
		각각의 크기가 O(d|V|)일 경우, 모델 파라미터 집합은 {Φ, T}임.
		SGD는 이 파라미터들을 최적화하기 위해 사용됨.
		역전파 알고리즘을 통해 도함수가 추정됨. 학습률 α는 학습 초기 2.5%로 설정되며, 각 시점까지 관찰된 꼭지점의 수에 따라 선형적으로 감소함.

	4.3 병렬성
	소셜 네트워크의 랜덤워크 내 꼭지점과 언어 내 단어의 빈도 분포는 멱법칙을 따름.
	이는 빈번하지 않은 꼭지점의 롱테일로 나타나는데, 그러므로 Φ에 영향을 미치는 업데이트는 자연히 희소해짐.
	이는 다수 컴퓨팅 프로세서 상황에서 SGD의 비동기화된 버전을 사용할 수 있게 함.

	우리의 업데이트가 sparse하고 모델에 공유된 파라미터에 접근하지 못할 때,
	ASGD는 수렴의 최적화율을 달성함.
	멀티 스레드 머신에서 연구를 수행할 동안, 이 기법이 매우 확장가능하며, 아주 큰 규모의 머신러닝에 사용될 수 있다고 증명되었음.

	그림 4는 딥워크 병렬화의 효과를 보여줌. 
	BlogCatalog와 Flickr 네트워크를 처리함에 있어 프로세서의 수를 8개까지 늘림에 따른 가속이 꾸준함을 보여줌. 


	4.4 알고리즘 변형
	제안된 방법론의 흥미로울 것으로 생각되는 몇몇 변형을 논의함.

		4.4.1 Streaming
		이 방법론의 한 흥미로운 변형은 streaming 접근인데, 이는 전체 그래프에 대한 지식 없이 사용될 수 있음.
		이 변형에서는, 그래프로부터 얻은 작은 워크를 표현학습 코드에 직접적으로 통과시켜 주고, 모델이 직접적으로 업데이트 됨. 
		이 학습 과정에 대한 몇가지 보정이 필요할 것임.
		먼저, 감쇠 학습률 (decaying learning rate)이 더이상 가능하지 않음. 대신, 학습률 α를 작은 상수 값으로 사용해 시작할 수 있음.
		이는 학습에 더 오랜 시간이 걸리지만, 몇 응용에서는 사용할 가치가 있을 수 있음.
		두번째로, 더이상 파라미터의 트리를 꼭 생성할 수 있는 것은 아님.
		V의 개수가 알려져 있다면(혹은 고정될 수 있다면) 해당 최대 값에 상응하는 계층적 소프트맥스 트리를 만들 수 있음.
		꼭지점은 처음 발견되었을 때 남아있는 잎들 중 하나에 할당될 수 있음.
		만약 꼭지점 빈도의 사후확률을 추정할 능력이 있다면, 허프만 코딩을 사용해 빈출 원소의 접근 시간을 줄일 수 있음.

		4.4.2 비-무작위적 워크
		몇몇 그래프들은 원소의 시퀀스와 상호작용하는 에이전트의 부산물로써 생성될 수 있음 (웹사이트 상 유저의 네비게이션)
		그래프가 이러한 일련의 비-무작위적 워크에 의해 생성된다면, 모델링 단계에 직접적으로 통과시키는 데 이 과정을 사용할 수 있음.
		이러한 방식으로 샘플링된 그래프는 네트워크 구조와 관련된 정보를 포착할 뿐만 아니라, 어떤 경로로 순회 되었는지에 대한 빈도도 잡아낼 수 있음.

		우리의 관점에서, 이러한 변형은 언어 모델링을 망라함.
		문장들은 적절히 설계된 언어 네트워크에 걸친, 목적성 있는 워크로 볼 수 있고,
		SkipGram같은 언어 모델은 이러한 행동을 잡아내기 위해 고안됨.

		이러한 접근은,
		전체 그래프를 명시적으로 건설할 필요 없이, 연속적으로 진화하는 네트워크의 특징을 학습하기 위해 4.4.1에서 고안된 streaming 변형과 결합될 수 있음.
		이런 기법과 함께 representation을 유지하는 것은, web-scale 그래프를 귀찮게 다룰 필요 없이 web-scale 분류를 가능케 함.


