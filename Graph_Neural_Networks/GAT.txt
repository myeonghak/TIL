Graph Attention Network



Motivation
	1) 데이터는 그래프의 형태로 표현될 수 있음
	2) 이러한 노드의 표현을 분류, 회귀, 추천 등의 태스크를 위해 활용할 수 있음
	3) 표현(representation)을 생성하기 위해, 노드의 이미지/텍스트/기타 피처 등을 활용할 수 있음
	4) 그러나 기존의 deepwalk같은 연구에서는, graph structure만을 주목하여 봄으로써 활용할 수 있는 정보에 한계가 있었음
	5) 이 논문에서는, graph structure 뿐만 아니라 text content를 사용해 고품질의 representation을 얻어냄.

1.Model Architecture
	1) start with original feature:
	한 중심 노드가 주어지면, 해당 노드를 중심으로 주변의 이웃 노드를 연관지음
	
	2) linear transformation:
	각 노드(h)는 텍스트/이미지 정보를 담고 있음. 그런데, 이 정보를 주어진 차원 그대로 bag of words 방식 등으로 활용하기에는 너무 비효율적임. 이를 개선하기 위해, weight matrix를 가운데 두어 compact한 representation으로 만들어 줌. 이렇게 만든 representation h’을 이웃들에게 전달.
	
	3) evaluate attention:
	B기준으로, 각각의 이웃 A,B,C,D에 대한 e_B_A부터 쭉 구해 줌. 이는 leakyRelu 함수를 통과한 값으로 나옴. 여기에 softmax를 취해 줌으로써 attention score가 결정됨.
	
	4) Summation:
	이 결과를 모두 총합 하여 h’’을 얻어냄.
	
	5) Multi-head attention:
	입력 차원을 head 수만큼 쭈새어 각 부분을 분담하게 함. 이 결과를 모두 concat
	
	6) Multi-head attention for the final layer
	
	7) cross-entropy loss function. 