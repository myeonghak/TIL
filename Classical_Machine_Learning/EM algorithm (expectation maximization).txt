EM algorithm(Expectation Maximization)



1. 변수가 분포되어 있을때, 어떤 변수가 어떤 라벨을 갖는지 알 수 없음. 이때 k개의 각 라벨이 각각 다른 모수를 갖는 가우시안 분포를 따른다고 가정할 경우, 특정 점이 어떤 가우시안을 따를지 확률값을 구할 수 있음(베이지안 정리)

2. 그러나 이 k개 가우시안의 모수를 알수 없음. 그래서 등장한 것이 EM 알고리즘이다.

3. k개의 가우시안 분포를 무작위로 놓고 시작한다. 각 데이터 포인트에 대고, 이 분포(b)에서 왔을 확률이 얼마나되지? 하고 P(b|xi)를 계산함.

4. 하지만 k-means와는 달리, 강한 할당(hard assignment)를 하진 않음. 대신에, 확률값을 구해 줌. never quantize, 즉 0 or 1이 되지 않고 항상 그 사이에서 맴돈다. (soft assignment) 

5. 그 뒤, 할당된 값에 적절하도록 모수를 재 추정한다. 

6. 이 과정을 convergence까지 반복



https://youtu.be/REypj2sy_5U



19.12.16