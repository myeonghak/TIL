SVM 2 - Softmargin SVM, Nonlinear SVM, kernel

1. Soft margin SVM: 
	선형 hyperplane으로 완벽하게 분리할 수 없는 경우 slack variable xi라는 오차를 허용해 주어야 함. 이를 도입함으로써 training error를 허용해 줌. 이로써 우리가 구해야할 decision variable은 w,b,xi가 됨.
	
2. 이는 margin을 maximize하는 (수식상으로는 w의 l2-norm을 minimize) obj func에 penalty로써 작용함. 
	이 때 xi의 계수인 C는 margin과 training error 사이의 trade-off를 결정하는 tuning parameter임. 
	C가 크면 xi를 많이 허용하지 않으므로 overfitting의 위험이 있고, 반대로 작으면 trainin error를 많이 허용하므로 underfitting의 위험이 있음
	
3. Lagrangian Formulation
	: 원래의 제약식이 딸린 최적화 문제(original problem)을 Lagrangian multiplier를 도입하여 Lagrangian Primal 문제로 변환, 이제 식은 max|min L(alpha,w,b,xi,gamma)로 바뀜. 
	여기서 alpha와 gamma는 라그랑지 승수임.
	
4. Hard margin (linearly separable) case와 마찬가지로, decision variable은 alpha이고, 앞의 수식을 w,b,xi에 대해 미분해 얻은 관계식을 바탕으로 alpha의 solution을 구할 수 있음.

5. 이 때 alpha의 범위가 Hard margin에서는 0<=alpha였으나 Soft margin에서는 0<=alpha<=C가 됨.

6. 이제 KKT 조건에서 가장 중요한 4) complementary slackness를 이용해, 최종적인 답에 대한 성질을 살펴봄. 

7. alpha_i*(y_i(w^Tx_i+b)-1+xi_i)=0,
	alpha=C-gamma_i,
	xi_i*gamma_i=0
	의 식에서, 곱의 형태의 두 항의 조건에 따라 각 데이터포인트가 어떤 의미를 갖는지 살펴봄.
	
	1) alpha_i=0, 따라서 gamma_i=C->xi=0, 그로 인해 맨 위 식에서의 우항은 0이 아니게 됨. 즉, “x_i가 plus plane/minus plane 위에 있지 않음”
	2) 0<alpha_i<C -> gamma_i>0 -> xi=0 -> 그로 인해 맨 위 식에서의 우항은 0이 됨. 따라서 “x_i가 plus plane/minus plane 위에 있으며, 즉 x_i는 support vector가 됨”
	3) alpha_i=C -> gamma_i=0 -> xi_i>0 -> 맨 위 식에서의 우항은 0이 아님. 따라서 “x_i가 plus plane/minus plane 사이에 있는 support vector가 됨”
	
8. 결국 gamma와 xi가 추가되었으나 앞서 살펴본 linearly seperable 상황에서의 방식과 동일하게 margin을 구할 수 있음.

9. 지금까지 hard margin, soft margin은 선형으로 분리할 수 있는 경우와 없는 경우에 각각 선형 SVM에 대해 설명함. 
	(bayes decision boundary는 후자의 경우 비선형적인 형태를 띠지만, 가운데 hyperplane은 선형임.) 이제는 비선형 SVM에 대해 설명함
	
10. 선형 SVM으로 완전히 구별되지 않는 데이터를 “고차원으로 변환함으로써” 비선형적으로 구분해보자. 

11. 기본적인 아이디어
	1) p<q인 q차원으로 x를 변환하는 함수를 phi(x)라고 한다. 이 phi(x)를 통해 데이터 x를 고차원(q차원) 공간으로 맵핑
	2) 이 공간 내에서 linear decision boundary를 구함
	3) 이 decision boundary를 저차원(p차원) 공간에 projection함. 이를 사용하면 비선형적 결정 경계를 얻을 수 있음
	
12. 이처럼 data representation을 바꾸거나, 좌표계를 변환함. 물결무늬를 갖는 좌표계에서는 같은 굴곡의 물결무늬가 선형일 것임. 

13. 정리하자면, original space가 아닌 feature space에서 SVM을 학습함. original에서는 비선형인 결정경계가 feature에서는 선형일수 있음. 
	고차원 공간에서는 관측치 분류가 더 쉬울수도 있음. 이 고차원 feature space를 효율적으로 계산할 수 있는 방법이 있음
14. kernel mapping: SVM Lagrangian dual formulation에서, x^Tx가 나옴. (내적) 우리의 아이디어는 이 x를 phi(x)로 변형해서 사용하겠다는 말임. 
	그런데, 여기서 phi의 구체적인 형태를 몰라도 <phi(x_i), phi(x_j)>의 형태가 그대로 구현된 어떤 함수를 사용해도 똑같은 효과를 기대할 수 있을 것임. 이 어떤 함수를 kernel function이라고 함. 
	
15. 여기서 커널함수 K(x_i, k_j)는, 아래와 같은 질문의 해답임
	Q: X, Y라는 데이터 포인트가 있고, 이를 고차원 공간으로 맵핑하는 phi(x)가 있을 때, <phi(X),phi(Y)>의 결과값을 phi를 explicit하게 알지 못하더라도 계산할 수 있는가?
	-> yes! 
	
	그 해답이 kernel function이며, 이는 ‘can be obtained without knowing  explicit form of phi(x) and phi(y)’임. 
	여기서 ‘without knowing explicitly’는 implicit하게 안다는 말이며, 이때문에 kernel trick이라는 말이 생겼음.
	
16. 커널 함수의 종류에는 
	- linear kernel
	- polynomial kernel- sigmoid kernel (hyperbolic tangent kernel)
	- Gaussian kernel (radial basis function (RBF) kernel)
	가 있음.
	보통은 sigmoid/gaussian kernel을 많이 씀. 4차원 미만의 polynomial kernel도 종종 사용됨.
	
17. 데이터 타입에 따라 최적의 커널함수는 다를 것이므로, 일종의 하이퍼파라미터로 생각할 수 있음.

