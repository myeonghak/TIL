[TIL] 기계학습 강좌 1-3. MAP



1. 사전 지식의 결합(Incorporating prior knowledge)
	- 베이지안적인 접근법
	- 앞에서의 압정 예시를 그대로 들고옴. 베이즈가 등장해서, 0.6이라는 확률이 맞는가? 50:50이 아닐까? 하는 의문을 제기하게 됨.
	- 여기서 50:50일거라는 의견은 사전지식으로, 이를 결합하여 추정에 활용할 수 있음을 주장함
	- P(theta|D)=P(D|theta)*P(theta)/P(D)
	- Posterior=Likelihood*Prior Knowledge/Normalizing Constant
	- 앞서, MLE 접근에서 P(D|theta)를 구했음. 여기에 P(theta)라는 사전지식을 가미해보면 좋을 것 같음.

2. 베이지안 관점에서의 더 많은 공식
	- P(theta|D) £(비례기호, proportion) P(D|theta)*P(theta) -> P(D)는 이미 주어진 상수이므로 소거하되, 등호를 비례기호로 바꾸어줌
	- P(D|theta)=theta^a_H*(1-theta)^a_T로 나타낼 수 있음
	- 이제 P(theta)를 나타내는 것이 관건임. 앞에서 나타낸 방식과 유사하게, 특정 분포에 의존해서 표현해야함.
	- 여기서 베이즈는 beta dist.를 쓸 것을 제안함.
	- 베타 분포는 0-1사이에 cdf(cumulative density function)가 confine되어 있어 확률을 나타내기 용이함.
	- 이 분포에서는 alpha와 beta라는 parameter를 입력으로 받음.
	- 이 식을 정리하면, P(D|theta)*P(theta)을 나타낼 수 있음. P(theta)항의 B(alpha, beta)는 theta에 독립적인 constant term이므로 이는 위에서 P(D)를 처리한 것과 같이 proportion으로 처리할 수 있음. (소거됨)
	- 정리하면, MLE의 식과 유사한 꼴이 됨!

3. Maximum a Posteriori Estimation
	- MLE에서는 likelihood를 최대화했지만, MAP에서는 사후확률을 최대화하는 전략을 취함!
	- 즉 MLE는 P(D|theta)를 극대화하는 theta^hat을 찾고,MAP는 P(theta|D)를 극대화하는 theta^hat을 찾음
	- 동일한 방식으로 log 변환 후 미분하여 극점을 사용한 최적화를 수행함
	- 때문에 결과값은 MLE나 MAP나 동일함. 관점이 다른 것임!
	-MAP에서 구한 공식에서는 alpha, beta에 대한 사전지식을 반영하여 theta^hat의 값을 변화시킬 방법이 있음. 가령 회장이 alpha와 beta가 반반이라고 생각하면 이를 반영할 수 있음.
	- 그러나 MLE에는 그러한 방법이 없음.

4. 결론
	- 시행이 아주아주 많아지면, alpha와 beta의 영향은 점점 fade away하고 a_H와 a_T의 영향이 dominant해짐. 결국 MLE와 MAP의 결과는 같아지게 됨
	- 관측치가 많지 않을 경우에는 우리의 사전지식인 alpha와 beta를 반영할 수 있음. 올바른 도메인 지식에 의해 도출될 경우 유용할 것임!