Ridge Regression



1. 소수의 데이터를 통해 적합시킬 경우, 과적합되는 문제를 해결하기 위해, 데이터포인트에 둔감하게 변하도록 기울기에 패널티를 준 것이 능형 회귀(Ridge Regression)

2. 회귀는 물론 순서형 변수에도 사용할 수 있는데, 능형 회귀 역시 그렇다.

3. 로지스틱에도 능형회귀의 패널티가 사용될 수 있다!

4. y 절편에는 물론 패널티 적용되지 않음

5. 3차원 회귀의 경우 적합모형은 2차원! (plane)

6. 변수 선택에 사용되는 원리
	: n개의 피처가 있을 때, n개의 샘플이 필요하다. 이 경우 데이터가 너무 적으면 최적합선을 찾기 불가능. 그러나 패널티 식에 모두 때려넣으면? 가능! 이때, CV를 사용하여 최적합선을 찾는다.

7. 최적 람다도 CV로 찾는다

8. LS 방식보다 bias는 높지만 variance는 낮출 수 있게 됨.

9. 여기서 람다는 0~양극한

10. Lasso Regression

11. 제약식: lambda*|slope| 

12. 여기서도 람다는 0~양극한

13. 람다를 늘릴수록, 기울기가 줄어드는 현상이 발생. 특히, ridge의 경우 변수의 계수가 0이 되지는 않으나 Lasso의 경우 계수를 0으로까지 낮출 수 있음. 이러한 원리때문에 변수 선택에 사용할 수 있음! 불필요한 변수의 계수를 0으로 만들어 없애버림

14. 능형회귀에 비해 모델 분산을 낮추는 데 좋음

15. 따라서 능형은 모든 변수가 유용할 때 성능이 좋고, 라쏘는 불필요한 변수가 섞여있을 때 성능이 더 좋다

16. 능형의 경우 제곱을 씀. 이 경우엔 SSE가 늘어나는 양보다 더 증가할 수 없기 때문에, 0이 될수는 없음. 하지만 라쏘의 경우 절대값이기 때문에 변동분이 정해진 양임. 따라서 0이 가능?

https://stats.stackexchange.com/questions/151954/sparsity-in-lasso-and-advantage-over-ridge-statistical-learning


Elastic Net Regression

17. lasso와 ridge를 합침. 단순히 람다1*기울기 제곱+람다2*기울기 절댓값을 제약식으로 사용함

18. 여기서 람다들은 각기 다른 값을 가짐

19. 변수간에 상관관계가 있을 때 특히 유용함. 이는 라쏘의 경우 상관관계가 있는 변수 중 하나를 선택하고 나머지는 제거하고, 능형의 경우 모든 상관관계가 있는 변수들의 파라미터를 줄여주는 경향이 있기 때문.
따라서 Elastic net은 상관이 있는 변수들을 그룹핑하고, 그 그룹을 없애버리거나 남겨두거나 하는 선택을 내림!


19.12.13