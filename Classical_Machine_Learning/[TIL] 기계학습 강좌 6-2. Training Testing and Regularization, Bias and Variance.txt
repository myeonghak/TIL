[TIL] 기계학습 강좌 6-2. Training Testing and Regularization, Bias and Variance



1. Source of Error in ML
	- 머신러닝 모델의 에러에는 두가지 원인이 있음: Approximation(근사)과 Generalization(일반화)
	- E_out <= E_in + omega
	- E_out은 estimation error임. 모델이 필연적으로 갖는, 선형회귀의 경우 에러텀을 나타냄
	- E_in은 Approximation error로, 학습 알고리즘에 의해 유발됨
	- omega는 관측치의 분산에 의해 발생함(즉, 테스트셋의 분산이 트레인 셋의 분산보다 더 커서 발생, 한번도 보지 못한 데이터가 테스트셋에 포함되는 경우로 인해 발생)

2. 추가적인 기호 정의
	- f: 학습할 타겟 함수
	- g: ML 알고리즘의 학습 함수
	- g^(D): 데이터셋을 사용해 학습된 함수, 혹은 가설의 한 인스턴스
	- D: 실세계에서 뽑혀 나온 사용 가능한 데이터셋
	- g^hat: 무한한 수의 dataset이 주어졌을 때의 average hypothesis. 수식적으로, g^hat(x)=E_D[g^(D)(x)]

3. Bias and Variance
	- E_out <= E_in + omega
	- E_out(g^(D)(x))=E_x[(g^(D)(x)-f(x))^2]을 정리해 나가면 다음과 같이 단순화 가능
	- E_D[E_out(g^(D)(x)]=E_x[E_D[(g^(D)(x)-g^hat(x)^2)]+(g^hat(x)-f(x))^2]의 수식이 도출됨
	- 이는 크게 두 부분으로 나눌 수 있음
	1) Variance: E_D[(g^(D)(x)-g^hat(x)^2)]. 현재 주어진 데이터셋에 specialize 되어 있어, 미래에 들어온 데이터의 variance를 충분히 cover하지 못하기 때문에 발생하는, generalization 관점에서의 에러. 수식적으로 살펴보면, “우리가 가진 데이터셋으로만 만든 모델”과 “무한한 데이터셋을 전부 다 봤을 때 만들어질 모델”의 차이를 계산한 것임
	2) Bias: (g^hat(x)-f(x))^2. 우리가 가진 모델의 한계점에 의해 발생하는 에러. 수식적으로는, “무한한 데이터를 전부 다 봤을 때 만든 모델”과 “실제 함수”와의 차이임. 실제 함수를 모사하여 모델을 만들 뿐이고, 실제 모델과는 차이가 있을 수밖에 없음.

4. Bias-Variance 2
	- Variance: 모델이 average hypothesis가 되도록 학습할 수 없음을 나타냄. 이는 데이터셋의 한계에 의해 발생함
	- Bias: 실제 세계에 부합하는 average hypothesis를 학습할 수 없음을 나타냄. 이는 모델의 복잡도가 충분하지 않을 경우 발생함

5. Bias-Variance trade-off
	- Bias를 줄이는 방법: 데이터 분포가 있을 때, 선형인 모양을 보고 이 모델은 선형일거야! 라는 생각으로 모델을 피팅하면, 실제 데이터 분포와 차이가 있기 때문에 bias가 발생함. 따라서 더욱 복잡한 모델을 고안함으로써 모델의 bias를 줄일 수 있음
	- Variance를 줄이는 방법: 더 많은 데이터를 모델 학습에 사용함으로써 줄일 수 있음
	- model complexity가 증가하면 bias는 줄지만 variance는 커짐
	- 위의 bias+variance 수식이 나타내는 바는 하나가 줄어들면 다른 하나는 커질 수 밖에 없음을 나타냄