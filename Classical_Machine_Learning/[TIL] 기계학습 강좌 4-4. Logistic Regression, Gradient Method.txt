[TIL] 기계학습 강좌 4-4. Logistic Regression, Gradient Method



1. Taylor Expansion
	- taylor series(테일러 급수)는 함수의 한 representation임.
	- 고정된 지점에서 함수의 도함수 값으로부터 계산된 항의 무한한 총합임
	- f(x)=f(a)+(f’(a)/1!)*(x-a)+ (f’’(a)/2!)*(x-a)^2+...+ (f^(n)(a)/n!)*(x-a)^n
	- 여기서 a는 상수(x축 위의 한 지점을 의미)
	- 테일러 급수는 실수/복소수 a에 대해 무한히 미분 가능한 함수일 때 가능
	- 테일러 전개는 테일러 급수를 구하는 과정임
	- 테일러 전개를 무한히 하면 정확한 함수 f(x)를 구할 수 있음.
	- 현실적으로 일부의 차수만큼을 전개하면 상당히 유사한 함수로 근사할 수 있음

2. Gradient Ascent/Descent
	- 기울기 상승/하강 알고리즘은, 미분 가능한 함수 f(x)와 시작점 파라미터 x_1이 주어졌을 때, 해당 파라미터를 f(x)의 더 높은/낮은 값으로 반복적으로 이동하는 방법임. f(x)의 positive/negative한 기울기의 방향을 취함으로써 성취
	- 아주 넓은 parameter space에서 처음 주어진 초기점 x_1을 움직이는 것임.
	- 이동에는 방향과 속도가 필요함. 그러나 중요한 것은 방향임. GD 방법의 핵심은 방향을 제공해 주는 것임. 속력은 사용자가 hp로 가짐.
	- f(x)=f(a)+(f’(a)/1!)*(x-a)+O(||x-a||^2), 여기서 bigO notation의 의미는 이 이후의 함수의 값들은 bigO내의 gross에 confine되어 있다는 의미.
	- 여기에 들어가는 a의 값은 앞서 언급된 초기값 x_1이라고 가정. x=x_1+h*u인데, u는 방향을 가지는 단위 벡터(unit (direction) vector)로 편미분 값의 방향을 나타냄. h는 속력
	- 이 때 어떻게 u라는 unit direction vector를 잘 정해줄까?

3. unit direction vector 결정
	-f(x_1+h*u)=(테일러전개 사용) f(x_1)+h*f’(x_1)*u+h^2*O(1)
	- 여기서 h는 0.01과 같은 아주 작은 값이고 O(1)는 상수로 취급할 수 있기 때문에, f(x_1+h*u)-f(x_1)~=h*f’(x_1)*u로 정리 가능. 항상 되지는 않음, h를 잘 결정함으로써 이를 성취할 수 있음! (왜 lr을 0.001과 같이 작게 하는가? 에 대한 대답)
	- gradient descent의 경우
	: u*=argmin_u (f(x_1+h*u)-f(x_1))=argmin_u (h*f’(x_1)*u)= -f’(x_1)/|f’(x_1)|, 여기서 절대값은 단위벡터를 나타내기 위해 취해짐
	- argmin을 취해주어야 하기 때문에, 편미분 값의 반대방향 (-)으로 이동해 줌. 수식으로 나타내면 내적을 취한 형태가 되기 때문에  cosine alpha가 식에 들어가게 되는데, 이 cos alpha가 가장 작아지는 alpha는 180도 지점(pi)임.

4. 결과적으로 x_1-h*u(앞서 구한 단위 벡터 식)의 값이 x_1으로 업데이트 되고, 다음 루프에서는 그 값이 x_1 대신 들어가게 됨.

5. Gradient Ascent는 -를 +로 바꾸어 주기만 하면 됨.
	