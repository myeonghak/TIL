[TIL] 기계학습 강좌 4-5. Logistic Regression, How Gradient method works



1. 예제 함수: Rosenbrock 함수
	- f(x1,x2)로, (1,1)에서 global minimum이 나타남.
	- 이를 해석학적으로 풀면 trivial하게 풀 수 있지만, 이번에는 GD method를 설명하기 위해 approximation을 하려고 함

2. 충분한 iteration을 반복한 결과 극소점에 도달할 수 있었음

3. lr을 높이는 것은, 앞서 생략했던 bigO notation term을 무시할 수 없게 만들어버리므로 신중히 선택해야 함