[TIL] Logistic Regression 1 by 김성범



1. 로지스틱 회귀모델의 필요성
	- 반응변수가 연속형이 아닌 범주형
	- target 변수가 정규분포를 따르지 않을 것이고, 평균이 0 분산이 1이지도 않을 것임
	- 다른 접근을 취해야함
	- 새로운 관측치가 왔을 때 이를 기존 범주 중 하나로 예측(범주예측)

2. Y_i=beta_0+beta_1*X_i+epsilon_i, Y_i=0|1 (베르누이 확률변수)
	- assume E(eps_i)=0, E(Y_i)=beta_0+beta_1*X_i
	- P(Y_i=1)=phi_i
	- P(Y_i=0)=1-phi_i
	- E(Y_i)=1*phi_i+0*(1-phi_i)=phi_i
	E(Y_i)=beta_0+beta_1*X_i=phi_i
	- 이는 X값이 주어졌을 때 출력변수 Y가 1의 값을 가질 확률
	- 이런 구조를 띠면 관계식을 직선으로 모델링하는 것은 적합하지 않음.
	- X를 로지스틱 함수 형태(비선형결합)로 표현

3. 로지스틱 회귀모델-로지스틱 회귀함수
	- f(x)=1/(1+e^(beta_0+beta_1*X_i))
	- 입력값 X는 음극한부터 양극한까지 가능하지만 출력값은 0-1사이의 값을 가짐
	- 이를 logistic, sigmoid, squashing function이라고도 부름(큰 값에서 작은 값으로 찌부러뜨린다)
	- 인풋 값에 대해 단조증가/단조감소 함수
	- 미분 결과를 아웃풋의 함수로 표현 가능(gradient learning method에 유용하게 사용)
	-> f’(x)=f(x)*(1-f(x))
	- 관측치 X가 클래스 1에 속할 확률을 나타냄

4. 회귀계수 beta의 해석
	- 선형회귀에 비해 해석이 직관적이지 못함
	- Odds(승산): 성공확률을 p라 할때, 실패확률대비 성공확률. Odds=p/(1-p)
	- 로지스틱 회귀모형의 계수 beta의 해석을 용이하게 하기 위해 Odds라는 개념 도입
	- p=1이면 odds는 무한, p=0이면 odds는 0
	- 로지스틱 회귀에서 odds는 범주 0에 속할 확률 대비 범주 1에 속할 확률로 정의할 수 있음
	- Odds=phi(X=x)/(1-phi(X=x)), 분자의 x는 1/분모의 x는 0
	- 이렇게 구한 Odds에 log를 취해주면, beta_0+beta_1*X라는 선형결합식이 나옴
	- 이렇게 변형한 식은 해석이 더욱 직관적으로 변함. X가 1만큼 증가했을 때 log(odds)의 변화량으로 해석할 수 있음.
	- 즉, X가 1만큼 증가했을 때 성공확률(클래스1일 확률)이 log scale로 얼만큼 증가했는지를 알 수 있는 것임.

5. 로짓 변환(logit transformation)
	- 확률변수에 
	1) Odds를 구해서
	2) log를 취해주는 변환