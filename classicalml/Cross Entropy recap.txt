Cross Entropy Recap

https://youtu.be/ErfnhcEV1O8

1. loss function으로 가장 흔하게 사용되는 함수임.
2. entropy에 대해서 먼저 살펴 봄. 이는 정보이론에서 등장하는 개념으로, 정보의 불확실성의 정도를 나타냄.
3. 만약 날씨가 맑음/흐림 두가지가 있고 이들이 각각 50%의 확률을 가진다고 할때, 기상청이 우리에게 전달할 정보는 1 bit로 표현이 가능함 (0/1)
4. 그런데 8개의 균일 확률에서 발생하는 날씨상태를 가지고 있다면, 총 2^3=8이므로 log2(상태수 8)을 의미하는 3 bit만큼이 필요하게 됨. (000부터 111까지 총 8개를 표현할 수 있으므로)
5. 이때 만일 확률이 다르다면? 맑음이 75%, 흐림이 25%일 경우를 생각해보자. 만일 기상청이 내일 비가 온다고 했을 경우, 나의 불확실성은 4배만큼 떨어지게 되고 이는 정보로 표현하면 2비트임.
6. 불확실성의 감소량은 사건확률의 역수임. 이 예시에서 25%의 역수는 4임. 
7. log(1/x)는 -log(x)와 같기 때문에, 비트의 수를 계산하는 식은 확률에 마이너스 이진로그를 취해준 것과 동일해짐. 따라서 -log2(25%)=2
8. 만일 기상청이 내일 해가 뜬다고 말해준다면, 불확실성이 이전만큼 크게 줄어들지는 않음. -log2(75%)=0.41
9. 정리하자면, 엔트로피는 어떤 사건이 얼마나 불확실한가를 표현하는 값이 됨. 날씨에 대한 정보를 받았을 때 획득한 정보의 양을 의미
10. 더 일반적으로는 주어진 확률분포 p에서 한 샘플을 뽑았을 때 획득하는 평균적인 정보의 양을 의미함. 이는 그 확률분포가 얼마나 불확실한지를 나타내 줌.
11. 만약 사막 한가운데 산다면 날씨에 대한 정보는 (평균적으로) 정보량이 매우 작을 것임. 그러나 날씨가 매우 불규칙적으로 변화하는 지역에서는 날씨의 정보량이 매우 클 것임. 따라서 엔트로피가 큼.
12. 이제 8가지의 날씨를 갖는 상황을 생각해보자. 수신자에게 이 날씨를 단순히 표현하면, 000~111으로 3비트의 정보로 전송할 것임. 그런데 확률을 고려해보면, 이는 매우 비효율적임. 맑거나 흐리거나가 70%고 나머지는 1% 수준인 것도 있음. 이 경우 평균적인 엔트로피(여기서는 다중의 상황이므로 크로스 엔트로피)를 구하면 2.23인데, 3비트를 보내서 쓰고 있으니 이는 매우 비효율 적인 셈임. 보내는데는 3비트나 들지만, 정작 받는 사람은 2.23비트의 정보만을 사용하는 것임
13. 이를 개선해보자. 각각 35%의 확률을 갖는 메인 사건 2개는 00,01로 인코딩하고 희귀한 사건은 11100처럼 인코딩하면 더 효율적임. 이렇게 처리한 뒤 다시 크로스엔트로피를 구하면, improved Cross Entropy는 2.42가 나옴!
14. 이는 우리가 목표로하는 2.23보다는 비싸지만 괜찮은 결과임.
15. 우리가 사용하는 코드, 즉 인코딩 방식은 날씨라는 사건의 분포에 대한 암묵적인 가정에 기반하기 때문임.
16. 예를 들어, 맑음에 2-bit 인코딩을 한다면 이는 우리가 암묵적으로 2^2일마다 한번씩 맑을 것이라 기대하는 것임. 적어도 평균적으로 그렇다고 기대하는 것.
17. 이러한 코드를 사용함으로써 우리는 맑음이라는 사건에 대해 25%의 확률을 가질 것이라 암묵적으로 예측하는 꼴이 됨. 이것이 틀리면 우리의 코드는 optimal하지 않은 것이 됨.
18. 우리의 예측(인코딩 방식에 의해 암묵적으로 수행된) q와 실제 분포 p는 차이가 있음. 
	이를 표현해 주는 것이 cross entropy임. 엔트로피의 식과 달리 log앞에 q가 붙어 있음. 
	만일 예측과 실제 분포가 완전히 동일했으면 CE는 Entropy와 동일할 것임. 
	동일하지 않다면, 크로스엔트로피는 엔트로피보다 몇 비트정도 더 클 것임. 
	엔트로피를 초과하는 크로스엔트로피의 정도는 relative entropy라고 불리우며, 더 일반적으로는 KL divergence라고 부름.
	
19. 요약) Cross Entropy = Entropy + KL divergence
20. 달리 말해 CE에서 E를 뺀 것이 KLD가 됨. (여기서 E는 상수이므로, CE를 최소화하는 것은 KLD를 최소화하는 것이 됨)
21. 머신러닝에서는 여러 클래스가 있을 때, 정답 라벨이 [0,0,0,1,0,0]처럼 주어지고 예측이 [0.01,0.02,0.03,0.90,0.02,0.02]처럼 주어졌을 때 이 확률벡터의 차이를 측정하는 지표로 사용됨.
22. 보통 자연로그가 쓰이는데, 이는 이진로그와 별 차이 없음. log2(x)=loge(x)/log(2)이기 때문에 단지 상수로 나눠준 것 뿐이기 때문임
