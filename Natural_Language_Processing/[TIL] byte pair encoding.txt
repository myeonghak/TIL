[TIL] byte pair encoding

1. https://youtu.be/zjaRNfvNMTs

2. BPE: 문장이 있을 때 토큰으로 쪼갤 때 사용하는 알고리즘. subword tokenization이라고 불림.

3. lemmatization, stemming으로 단어를 처리하고 쪼갰지만, 항상 작동하는 것은 아님. 사전에 없는 새로운 단어가 등장할 경우(OOV) 제대로 처리할 수 없음

4. 가능하다면, 의미 단위로 쪼개어 새로운 단어가 등장하더라도 비슷한 의미를 갖는 것처럼 처리하면 좋을 것

5. nykomlingen이라는 단어가 있으면 ny-komling-en으로 쪼개어 각각의 토큰이 갖는 의미를 임베딩하면 유용할 것

6. 학습 방식: 텍스트가 주어졌을 때, 그 안에서 가장 빈번하게 등장하는 byte pair(문자 뭉치)를 찾음. 이를 특정 토큰으로 대체해주고, 이와같은 작업을 반복하여 원하는 vocab의 수가 얻어질 때까지 수행

7. 학습 시연
	-  [“low</w>”,”lower</w>”,”newest</w>”,”widest</w>”]라는 코퍼스가 있음
	- 먼저 1글자(byte)짜리 토큰을 정리, [d,e,l,i,</w>...]
	- 길이 2짜리 토큰의 빈도를 측정
	- 그 중 가장 빈도가 높은 토큰 (eg “es”)을 선택하고, 이를 vocab에 일단 추가
	- 이 “es” 토큰을 하나의 토큰으로 간주하면, 길이 2짜리 토큰 “es”+”t”가 빈도 9로 가장 높은 녀석이 됨. 이 토큰을 다시 vocab에 추가 (“est”)
	- 이를 한번 더 반복할 수 있음. “est</w>”를 얻는 것임. 
	- 이 작업 끝에, 길이 1짜리 토큰들과, “es”,”est”,”est</w>” 세 개의 토큰이 추가됨.
	- 이 절차를 반복해 원하는 단어가 vocab에 추가될 때까지 수행할 수 있음. 이는 hyperparameter임.

8. OOV 처리 방식
	- lowest라는 새로운 단어가 추가될 경우, 기존의 토큰기에서는 모른다고 할 것임. 우리의 알고리즘은 가능한 토큰을 새로운 단어에서 찾아, low+est</w> 두 조각으로 쪼갤 수 있을 것임.
	-powest라는 단어가 주어질 경우, <unk>+o+w+est</w>로 쪼갤 것임.

9. 지금까지 설명한 방식은 character level byte pair encoding임. byte pair encoding과는 엄밀히 다르다고 할 수 있음, 이는 알고리즘의 이름임.

10. 중국어, 일본어 등의 텍스트를 처리하기 위해서는 byte level encoding이 필요함.

11. word piece tokenization: 
	- 글자를 count에 기반해 결합했음. 
	- 이 count 대신에, probability를 사용하면 어떨까? 
	- “es” 토큰을 코퍼스에서 볼 확률과 “er”을 볼 확률이 각각 얼마나 될까? 어떤게 더 큰가?로 토큰화함.
	- 이 토큰 방식이 버트에서 사용됨.