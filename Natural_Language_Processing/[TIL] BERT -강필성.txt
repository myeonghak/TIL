[TIL] BERT -강필성



1. Bert(Bidirectional Encoder Representation from Transformer):트랜스포머의 인코더 파트만 사용해서, bidirectional한 representation을 학습하겠다는 의미

2. 두가지의 목적을 가짐. MLM, NSP로 

3. MLM(Masked Language Model): 랭귀지 모델 자체는 sequential한 모델임. 이전 정보를 받아 다음 단어를 예측하거나, 엘모의 경우에는 뒤의 단어를 받아와 앞을 예측하는 backward/forward를 따로 학습을 했었음. gpt는 트랜스포머의 디코더부분만을 사용해 마스킹된 뒷부분을 예측하는 과정에서 학습함. 반면에 버트는 가운데 구멍을 뚫어놓고, 그러므로 방향성이 존재하지 않은 문제를 해결하며 학습.

4. NSP(next sentence prediction): 두 문장이 주어졌을 때, 다음 문장이 contextual하게 다음에 등장할지 그렇지 않을지를 예측하며 학습함

5. 이렇게 학습한 프리트레인 모델에 간단한 레이어 하나만 붙임으로써 기존의 SOTA 모델의 성능과 비견할만한 우수한 성능을 보임.

6. 버트 모델 아키텍쳐
	- Multi-layer bidirectional Transformer encoder
	   1) L: number of layers(transformer block)
	   2) H: hidden layer
	   3) A: number of self attention heads
	- BERT_base:L/H/A-12/768/12, 전체 파라미터 110M, OpenAI의 GPT와 같은 모델 사이즈
	- BERT_large: 24/1024/16, 전체 파라미터: 340M

7. BERT- input/output representation
	- 버트를 다양한 다운스트림 태스크에 활용하기 위해, input representation은 단일 문장 혹은 한 쌍의 문장(Question-Answer)을 명확하게 나타낼 수 있음.
	- sentence: 실제 언어학적 문장(문법구조를 갖고, 의미를 갖는 문장)이라기보다는 임의의 길이를 가진 인접한 텍스트를 말함. 즉 언어학적 문장보다 길수도 있고 일부분일수도 있음.
	- sequence: 버트에 들어가는 input token sequence, 단일 문장 혹은 한 쌍으로 묶인 sentence를(버트에서의 sentence) 의미함. 

8. BERT input/output representation
	- m의 길이를 갖는 토큰 시퀀스 2개가 들어가는데, 일정 비율로 토큰을 masking 처리함. 
	- 입력의 처음에 [CLS]라는 토큰, 두 문장의 사이에서 두 문장을 구별해주는 [SEP] 토큰이 들어감. 
	- 만약 감성분류등의 binary classification을 한다면, 12개 레이어를 쭉 타고 올라가 마지막 레이어에 있는 [CLS] 토큰 하나의 hidden representation으로만 예측을 수행함
	- 각각의 n번 인덱스 위치에 대응하는 토큰은 버트가 나타내 주는 n번째 토큰의 final hidden vector임. 
	- next sentence prediction, masked ML 두가지를 동시에 학습시키는 것이 목적이 됨.

9. 버트의 3가지 입력
	- token embedding: WordPiece embeddings with a 30,000 tokens vocabulary, 즉 워드피스 3만 단어에 대한 임베딩을 학습, 각 토큰에 대응되는 임베딩 값이 나옴.
	- Segment embedding: 앞에 나온 문장인지, 뒤에 나온 문장인지를 전달
	- Position embedding: transformer와 동일

10. 버트의 3가지 입력 (2) segment representation
	- 0번 문장에는 [CLS] 토큰과 [SEP] 토큰을 모두 포함함.
	- 앞문장의 768(bert base의 경우)차원의 segment embedding과 뒷문장의 768차원 segment embedding을 모두 학습시켜야 함.
	- segment embedding도 주어지는 것이 아니라, 이 자체도 학습이 된다고 이해.

11. pretraining BERT-(1) MLM (masked languaged model)
	- 각 시퀀스의 15%가 [MASK] 토큰으로 대체됨
	- 각 마스킹된 단어를 예측함(인코더를 denoising하는 과정에서 전체 입력을 복원하는 방식보다는)
	- 인풋 시퀀스 중 일부가 마스킹된채로 트랜스포머 인코더 블럭을 지나, FCN+GELU+norm을 통과한 뒤 원래 마스킹 되기 전의 형태로 예측될 수 있도록 학습됨.
	- 엘모의 경우에는 LSTM을 사용해 구한 정방향과 역방향의 representation을 선형결합해서 최종 rep을 구했는데, 버트는 순차적으로 학습하는게 아니라 쌍방향으로, 실제 입력의 값을 마스킹한 뒤 모델 학습을 통해 잘 예측하도록 유도한 것이 버트의 첫번째 목적이 됨.
	- pretrain단에서 mask 토큰이 나오지만 finetuning에서는 나오지 않기때문에, mismatch가 발생함
	- 이를 해소하기 위해, 마스킹될 토큰의 80%는 마스킹을 해주고, 10%는 랜덤토큰으로 대체하고, 10%는 그대로 내버려 둠. 이러한 비율이 실험 결과 가장 좋은 성능을 나타냈다고 저자들이 주장함.

12. pretraining BERT-(2) NSP(Next Sentence Prediction)
	- GPT, ELMo등은 문장 단위로만 학습을 하고, 두개 이상의 문장에 대해 학습을 하진 않았음.
	- 두개 이상의 문장에 대한 표현이 필요한 다운스트림 태스크 처리를 위해, binarized next sentence prediction을 버트에서 처음 도입함
	- 2개의 시퀀스가 들어감. [CLS] seq1 [SEP] seq2 이런 식으로. 임베딩 레이어를 지나온 마지막의 [CLS] 토큰의 representation으로 NSP 수행.
	- [CLS] seq1 [SEP] seq2 y=0
	[CLS] seq1 [SEP] seq7 y=1
	의 방식으로 라벨링을 함.
	- 0/1을 50%/50%로 나누어 줌.
	- 이 단순한 구조에도 불구하고, QnA, NLI(Natural Language Inference, 뒤 문장이 entail인지 아닌지 판단)task에 큰 도움을 주었다고 주장.

13. 하이퍼파라미터 셋팅
	- maximum token length: 512
	- batch size: 256
	- optimzer: adam with lr 1e-4, beta1=0.9 bate2=0.999
	- L2 weight decay 0.01
	- 첫 1만 스텝에서 lr warmup, lr를 linear decay함
	- 모든 레이어에 0.1 drop out
	- GeLU activation 사용
	- BERT_base는 16TPU, BERT_large는 64TPU로 4일간 학습
	- 90%의 스텝동안 128의 시퀀스 길이로 pre train, 나머지 10% 스텝은 512의 시퀀스로 학습