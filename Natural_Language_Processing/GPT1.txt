GPT1



1. GPT는 12 레이어의 12개 어텐션 헤드 트랜스포머 디코더 모델임. 이는 거대한 양의 레이블 없는 텍스트 데이터로 사전학습된 언어모델로 파인튜닝을 통해 다른 텍스트 데이터를 활용할 수 있는가를 보여준 모델임.

2. semi-supervised learning 방식이 언어 학습에 잘 작동될 수 있음을 보여준 모델임.

3. books corpus dataset으로 학습됨. 이는 출판되지 않은 다양한 장르의 7000개의 책 코퍼스로 이루어져 있음.